{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "homework5.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jB1YDdMM8TIe"
   },
   "source": [
    "# Homework 5. Sequence Tagging with LSTM\n",
    "\n",
    "Welcome to Homework 5! \n",
    "\n",
    "The homework contains several tasks. You can find the amount of points that you get for the correct solution in the task header. Maximum amount of points for each homework is _six_.\n",
    "\n",
    "The **grading** for each task is the following:\n",
    "- correct answer - **full points**\n",
    "- insufficient solution or solution resulting in the incorrect output - **half points**\n",
    "- no answer or completely wrong solution - **no points**\n",
    "\n",
    "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\n",
    "\n",
    "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\n",
    "\n",
    "When writing code, make it readable. Choose appropriate names for your variables (`a = 'cat'` - not good, `word = 'cat'` - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\n",
    "\n",
    "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\n",
    "\n",
    "<font color='red'>**Important!:**</font> **before sending your solution, do the `Kernel -> Restart & Run All` to ensure that all your code works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MVRSVyJhH9Xt"
   },
   "source": [
    "## Task 1. Download resourses for your language (0.5 points)\n",
    "\n",
    "In this homework, you are going to improve the pos tagger for your native language that we have built during the Lab. In particular, you are going to add a character level model to capture the inner structure of the word. This should help to better predict a correct tag. If there are no available resources for your language, you can choose any other language. \n",
    "\n",
    "__What is your native language?__\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "To start with, import all the packages below."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Yygt6gOJevIr"
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_sequence, pack_padded_sequence, pad_packed_sequence, PackedSequence\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "from typing import List, Dict\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIDOXHRpIWAH"
   },
   "source": [
    "Get the pretrained word vectors for your native language from [Fasttext](https://fasttext.cc/docs/en/crawl-vectors.html). For this model, you need to choose __text__ vectors (not bin!). If you don't have them available for your language, you can choose any other language that you want. Put the link instead of the `...` in the cell below.\n",
    "\n",
    "__Are there FastText vectors available for your language? If yes, provide the link to it below.__\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "q1dBJUSQe6eb"
   },
   "source": [
    "!wget ..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Dmuzm7tD4EF"
   },
   "source": [
    "Replace `...` with the filename of your vector in the first line. Put the same name but without a `.gz` extension instead of the `...` in the last line."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7J3TIUs0fc0U"
   },
   "source": [
    "!gunzip ...\n",
    "!mkdir vector_cache/\n",
    "!mv ... vector_cache/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxuSASQaJNAz"
   },
   "source": [
    "Next, we need the data to train on. For this task, we are going to use [Universal Dependencies](https://universaldependencies.org/) data. It has labelled corpora for morphological tagging and syntax parsing for over than 70 languages. You need to choose your language from the official UD page, choose the treebank that you like and follow the GitHub link to it. Then, from GitHub, copy the link from the green \"Clone or download\" button and replace it in the cell below. \n",
    "\n",
    "Also, replace the name of your treebank in the `!mv` command.\n",
    "\n",
    "For example, if I choose the EDT treebank for Estonian from [here](https://universaldependencies.org/#estonian-treebanks), the GitHub link is going to be `https://github.com/UniversalDependencies/UD_Estonian-EDT.git` and the name of the treebank is `UD_Estonian-EDT`, which is the name of the repository.\n",
    "\n",
    "__Is there a UD treebank available for your language? If yes, provide the link to it below.__\n",
    "\n",
    "<font color='red'>Your answer here</font>\n",
    "\n",
    "Replace the `...` with the GitHub link to the repository that you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "BJBLZXIEgbbo"
   },
   "source": [
    "!git clone ..."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1n0J8ZoFEoMb"
   },
   "source": [
    "Replace the `...` with the name of the treebank that you've chosen."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HJqCd9yQggJn"
   },
   "source": [
    "!mkdir data/\n",
    "!mv ... data/"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8KqQMYpKzUJ"
   },
   "source": [
    "This part is moslty the same as in the [Lab 6](). \n",
    "\n",
    "**Don't forget to change the `VEC_PATH` and `DATA_PATH` variables to match your data!**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Wb4fsehMgkBd"
   },
   "source": [
    "PAD = '<PAD>'\n",
    "PAD_ID = 0\n",
    "UNK = '<UNK>'\n",
    "UNK_ID = 1\n",
    "\n",
    "VOCAB_PREFIX = [PAD, UNK]\n",
    "\n",
    "VEC_PATH = Path('vector_cache') / '...'\n",
    "DATA_PATH = Path('data') / '...'\n",
    "MAX_VOCAB = 25000\n",
    "\n",
    "batch_size = 64\n",
    "validation_split = .3\n",
    "shuffle_dataset = True\n",
    "random_seed = 42"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5DyDzC6ORzY"
   },
   "source": [
    "You can consult the Lab materials if you have any questions about the vocab classes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DGsPh253hDIs"
   },
   "source": [
    "class BaseVocab:\n",
    "    def __init__(self, data, idx=0, lower=False):\n",
    "        self.data = data\n",
    "        self.lower = lower\n",
    "        self.idx = idx\n",
    "        self.build_vocab()\n",
    "        \n",
    "    def normalize_unit(self, unit):\n",
    "        if self.lower:\n",
    "            return unit.lower()\n",
    "        else:\n",
    "            return unit\n",
    "        \n",
    "    def unit2id(self, unit):\n",
    "        unit = self.normalize_unit(unit)\n",
    "        if unit in self._unit2id:\n",
    "            return self._unit2id[unit]\n",
    "        else:\n",
    "            return self._unit2id[UNK]\n",
    "    \n",
    "    def id2unit(self, id):\n",
    "        return self._id2unit[id]\n",
    "    \n",
    "    def map(self, units):\n",
    "        return [self.unit2id(unit) for unit in units]\n",
    "\n",
    "    def unmap(self, ids):\n",
    "        return [self.id2unit(idx) for idx in ids]\n",
    "        \n",
    "    def build_vocab(self):\n",
    "        NotImplementedError()\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._unit2id)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "AMvi6s3lsFA3"
   },
   "source": [
    "class PretrainedWordVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        self._id2unit = VOCAB_PREFIX + self.data\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "d9mn3bWHhXER"
   },
   "source": [
    "class WordVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        if self.lower:\n",
    "            counter = Counter([w[self.idx].lower() for sent in self.data for w in sent])\n",
    "        else:\n",
    "            counter = Counter([w[self.idx] for sent in self.data for w in sent])\n",
    "\n",
    "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: counter[k], reverse=True))\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b_dTkVH7GBwO"
   },
   "source": [
    "Here, we introduce a character vocab that is going to store the mappings for individual characters rather than words."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jgIck3Z61HY8"
   },
   "source": [
    "class CharVocab(BaseVocab):\n",
    "    def build_vocab(self):\n",
    "        counter = Counter([c for sent in self.data for w in sent for c in w[self.idx]])\n",
    "        self._id2unit = VOCAB_PREFIX + list(sorted(list(counter.keys()), key=lambda k: (counter[k], k), reverse=True))\n",
    "        self._unit2id = {w:i for i, w in enumerate(self._id2unit)}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPOQxl5mPdK8"
   },
   "source": [
    "You can consult the Lab materials if you have any questions about building the dataset."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jOefas-mheLI"
   },
   "source": [
    "class Pretrain:\n",
    "    def __init__(self, vec_filename, max_vocab=-1):\n",
    "        self._vec_filename = vec_filename\n",
    "        self._max_vocab = max_vocab\n",
    "        \n",
    "    @property\n",
    "    def vocab(self):\n",
    "        if not hasattr(self, '_vocab'):\n",
    "            self._vocab, self._emb = self.read()\n",
    "        return self._vocab\n",
    "    \n",
    "    @property\n",
    "    def emb(self):\n",
    "        if not hasattr(self, '_emb'):\n",
    "            self._vocab, self._emb = self.read()\n",
    "        return self._emb\n",
    "        \n",
    "    def read(self):\n",
    "        if self._vec_filename is None:\n",
    "            raise Exception(\"Vector file is not provided.\")\n",
    "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
    "        \n",
    "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
    "        \n",
    "        if failed > 0: # recover failure\n",
    "            emb = emb[:-failed]\n",
    "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
    "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
    "            \n",
    "        # Use a fixed vocab size\n",
    "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
    "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
    "            emb = emb[:self._max_vocab]\n",
    "                \n",
    "        vocab = PretrainedWordVocab(words, lower=True)\n",
    "        \n",
    "        return vocab, emb\n",
    "        \n",
    "    def read_from_file(self, filename, open_func=open):\n",
    "        \"\"\"\n",
    "        Open a vector file using the provided function and read from it.\n",
    "        \"\"\"\n",
    "        first = True\n",
    "        words = []\n",
    "        failed = 0\n",
    "        with open_func(filename, 'rb') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    line = line.decode()\n",
    "                except UnicodeDecodeError:\n",
    "                    failed += 1\n",
    "                    continue\n",
    "                if first:\n",
    "                    # the first line contains the number of word vectors and the dimensionality\n",
    "                    first = False\n",
    "                    line = line.strip().split(' ')\n",
    "                    rows, cols = [int(x) for x in line]\n",
    "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
    "                    continue\n",
    "\n",
    "                line = line.rstrip().split(' ')\n",
    "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
    "                words.append(' '.join(line[:-cols]))\n",
    "        return words, emb, failed"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "r8i2PuW6iafP"
   },
   "source": [
    "FIELD_NUM = 10\n",
    "\n",
    "class Word:\n",
    "    def __init__(self, word: List[str]):\n",
    "        self._id = word[0]\n",
    "        self._text = word[1]\n",
    "        self._lemma = word[2]\n",
    "        self._upos = word[3]\n",
    "        self._xpos = word[4]\n",
    "        self._feats = word[5]\n",
    "        self._head = word[6]\n",
    "        self._deprel = word[7]\n",
    "        self._deps = word[8]\n",
    "        self._misc = word[9]\n",
    "\n",
    "    @property\n",
    "    def id(self):\n",
    "        return self._id\n",
    "\n",
    "    @property\n",
    "    def text(self):\n",
    "        return self._text\n",
    "\n",
    "    @property\n",
    "    def lemma(self):\n",
    "        return self._lemma\n",
    "\n",
    "    @property\n",
    "    def upos(self):\n",
    "        return self._upos\n",
    "\n",
    "    @property\n",
    "    def xpos(self):\n",
    "        return self._xpos\n",
    "\n",
    "    @property\n",
    "    def feats(self):\n",
    "        return self._feats\n",
    "\n",
    "    @property\n",
    "    def head(self):\n",
    "        return self._head\n",
    "\n",
    "    @property\n",
    "    def deprel(self):\n",
    "        return self._deprel\n",
    "\n",
    "    @property\n",
    "    def deps(self):\n",
    "        return self._deps\n",
    "\n",
    "    @property\n",
    "    def misc(self):\n",
    "        return self._misc\n",
    "\n",
    "\n",
    "class Sentence:\n",
    "    def __init__(self, words: List[List[str]]):\n",
    "        self._words = [Word(w) for w in words]\n",
    "\n",
    "    @property\n",
    "    def words(self):\n",
    "        return self._words\n",
    "\n",
    "class Document:\n",
    "    def __init__(self, file_path):\n",
    "        self._sentences = []\n",
    "        self.load_conll(open(file_path, encoding='utf-8'))\n",
    "\n",
    "\n",
    "    def load_conll(self, f, ignore_gapping=True):\n",
    "        \"\"\" Load the file or string into the CoNLL-U format data.\n",
    "        Input: file or string reader, where the data is in CoNLL-U format.\n",
    "        Output: a list of list of list for each token in each sentence in the data, where the innermost list represents \n",
    "        all fields of a token.\n",
    "\n",
    "        Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/utils/conll.py\n",
    "        Stanza is released under the Apache License, Version 2.0.\n",
    "        \"\"\"\n",
    "        # f is open() or io.StringIO()\n",
    "        doc, sent = [], []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if len(line) == 0:\n",
    "                if len(sent) > 0:\n",
    "                    doc.append(Sentence(sent))\n",
    "                    sent = []\n",
    "            else:\n",
    "                if line.startswith('#'): # skip comment line\n",
    "                    continue\n",
    "                array = line.split('\\t')\n",
    "                if ignore_gapping and '.' in array[0]:\n",
    "                    continue\n",
    "                assert len(array) == FIELD_NUM, \\\n",
    "                        f\"Cannot parse CoNLL line: expecting {FIELD_NUM} fields, {len(array)} found.\"\n",
    "                sent += [array]\n",
    "        if len(sent) > 0:\n",
    "            doc.append(Sentence(sent))\n",
    "        self._sentences = doc\n",
    "\n",
    "    \n",
    "    @property\n",
    "    def sentences(self):\n",
    "        return self._sentences\n",
    "\n",
    "\n",
    "    def get(self, fields, as_sentences=False):\n",
    "        \"\"\"Taken and modified from Stanza: https://github.com/stanfordnlp/stanza/blob/master/stanza/models/common/doc.py\n",
    "        Stanza is released under the Apache License, Version 2.0.\n",
    "        \"\"\"\n",
    "        assert isinstance(fields, list), \"Must provide field names as a list.\"\n",
    "        assert len(fields) >= 1, \"Must have at least one field.\"\n",
    "\n",
    "        results = []\n",
    "        for sentence in self.sentences:\n",
    "            cursent = []\n",
    "            units = sentence.words\n",
    "            for unit in units:\n",
    "                if len(fields) == 1:\n",
    "                    cursent += [getattr(unit, fields[0])]\n",
    "                else:\n",
    "                    cursent += [[getattr(unit, field) for field in fields]]\n",
    "\n",
    "            # decide whether append the results as a sentence or a whole list\n",
    "            if as_sentences:\n",
    "                results.append(cursent)\n",
    "            else:\n",
    "                results += cursent\n",
    "        return results"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azsMK8kTSZfO"
   },
   "source": [
    "For the dataset, we are going to add the new `CharVocab` and preprocess each word character by character with it. For example, if you have a character vocabulary like this:\n",
    "\n",
    "`{'a': 0, 'b': 1, ..., 'y': 24, 'z': 25, 'A': 26, 'B': 27, ..., 'Y': 50, 'Z': 51}`\n",
    "\n",
    "Then a sentence `['I', 'like', 'cats']` is going to be transformed into `[[35], [11, 8, 10, 4], [2, 0, 19, 18]]`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h7UVFATAhsEV"
   },
   "source": [
    "class CONLLUDataset(Dataset):\n",
    "    def __init__(self, doc: Document, pretrain: Pretrain, \n",
    "                 vocab: Dict[str, BaseVocab] = None, test: bool = False):\n",
    "        self.pretrain_vocab = pretrain.vocab\n",
    "        self.test = test\n",
    "        data = self.load_doc(doc)\n",
    "\n",
    "        if vocab is None:\n",
    "            self.vocab = self.init_vocab(data)\n",
    "        else:\n",
    "            self.vocab = vocab\n",
    "\n",
    "        self.data = self.preprocess(data, self.vocab, self.pretrain_vocab)\n",
    "\n",
    "    def init_vocab(self, data: List) -> Dict:\n",
    "        wordvocab = WordVocab(data, idx=0)\n",
    "        charvocab = CharVocab(data, idx=0)\n",
    "        uposvocab = WordVocab(data, idx=1)\n",
    "        vocab = {\n",
    "            'word': wordvocab,\n",
    "            'char': charvocab,\n",
    "            'upos': uposvocab}\n",
    "        return vocab\n",
    "\n",
    "    def preprocess(self, data: List, vocab: Dict[str, BaseVocab], \n",
    "                   pretrain_vocab: PretrainedWordVocab) -> List[List[int]]:\n",
    "        processed = []\n",
    "        for sent in data:\n",
    "            processed_sent = [vocab['word'].map([w[0] for w in sent])]\n",
    "            processed_sent += [[vocab['char'].map([char for char in w[0]]) for w in sent]]\n",
    "            processed_sent += [vocab['upos'].map([w[1] for w in sent])]\n",
    "            processed_sent += [pretrain_vocab.map([w[0].lower() for w in sent])]\n",
    "            processed.append(processed_sent)\n",
    "        return processed\n",
    "        \n",
    "    def load_doc(self, doc: Document) -> List:\n",
    "        data = doc.get(['text', 'upos'], as_sentences=True)\n",
    "        return data\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J9L6KBcOkVu1"
   },
   "source": [
    "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BLhAK0nzIOhI"
   },
   "source": [
    "Put the correct path to your train file here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QraxfFjKs-jZ"
   },
   "source": [
    "train_doc = Document(DATA_PATH / '...')\n",
    "train_dataset = CONLLUDataset(train_doc, pretrain)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V4hOIwqtIUQq"
   },
   "source": [
    "Put the correct path to your dev file here."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Qdk0QJJheLpB"
   },
   "source": [
    "vocab = train_dataset.vocab\n",
    "dev_doc = Document(DATA_PATH / '...')\n",
    "dev_dataset = CONLLUDataset(dev_doc, pretrain, vocab=vocab, test=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPFocs3RIYTA"
   },
   "source": [
    "You can look inside the first sentence to see how the preprocessed data looks."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mBZSDFv6t_-I"
   },
   "source": [
    "train_dataset[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1_mmSj9Ipzv"
   },
   "source": [
    "We are going to pad the characters and save the original lengths for each word in a sentence to reconstruct the correct order later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lbhf2uYHvcJy"
   },
   "source": [
    "def pad_collate(batch):\n",
    "    (sents, chars, upos, pretrained) = zip(*batch)\n",
    "\n",
    "    sent_lens = [len(s) for s in sents]\n",
    "    word_lens = [len(c) for w in chars for c in w]\n",
    "\n",
    "    sents = [torch.LongTensor(w).to(device) for w in sents]\n",
    "    chars = [torch.LongTensor(c).to(device) for w in chars for c in w]\n",
    "    upos = [torch.LongTensor(u).to(device) for u in upos]\n",
    "    pretrained = [torch.LongTensor(p).to(device) for p in pretrained]\n",
    "\n",
    "    sent_pad = pad_sequence(sents, batch_first=True, padding_value=PAD_ID)\n",
    "    chars_pad = pad_sequence(chars, batch_first=True, padding_value=PAD_ID)\n",
    "    upos_pad = pad_sequence(upos, batch_first=True, padding_value=PAD_ID)\n",
    "    pretrained_pad = pad_sequence(pretrained, batch_first=True, padding_value=PAD_ID)\n",
    "\n",
    "    sent_pad = sent_pad.to(device)\n",
    "    chars_pad = chars_pad.to(device)\n",
    "    upos_pad = upos_pad.to(device)\n",
    "    pretrained_pad = pretrained_pad.to(device)\n",
    "\n",
    "    return sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pMNbcGrQ7dGd"
   },
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "li5RRsb6IrJO"
   },
   "source": [
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=shuffle_dataset, collate_fn=pad_collate)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CLNjeeQUa0U3"
   },
   "source": [
    "## Task 2. Build a character-level LSTM model (3 points)\n",
    "\n",
    "You already know that we can have a vector representation of a word that is learned from its context. We can use these vectors to capture sematical relations between the words. \n",
    "\n",
    "In addition to that we can learn an inner representation of a word, or its character-level embedding. This can help to capture morphological information about a word.\n",
    "\n",
    "We can do it by building another LSTM model and taking its last hidden state which is going to be the character-level representation of a word. The input to the model is going to be a stream for characters for each word in a batch.\n",
    "\n",
    "### Task 2.1. Define an Embedding layer (0.5 points)\n",
    "\n",
    "Create an [`nn.Embedding`](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) layer. The input size, or the number of embeddings, should be the size of our vocabulary (`CONLLUDataset` class shows how to access them). The output size, or the size of each vector, is `char_emb_dim`. Padding index is `0`. \n",
    "\n",
    "### Task 2.2. Define an LSTM layer (0.5 points)\n",
    "\n",
    "Create an [`nn.LSTM`](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM) layer. The input size should be the output size of the embedding layer. The hidden size is `char_hidden_dim`. The number of layers is `char_num_layers`. Set the `batch_first` parameter to `True`. Make the droupout to be `0` if number of layers is `1` or `droupout` otherwise.\n",
    "\n",
    "### Task 2.3. Embed the input (0.5 points)\n",
    "\n",
    "Now, you will implement a forward pass.\n",
    "\n",
    "Run the character input (`chars_pad`) through the embedding layer. Apply the dropout to the embeddings and save the result into the `char_emb` variable.\n",
    "\n",
    "### Task 2.4. Apply the LSTM layer (0.5 points)\n",
    "\n",
    "Put the embeddings into the LSTM layer. You are going to save only the last hidden state that is going to be the representation of the word."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SSY9a1DGHU8v"
   },
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    def __init__(self, vocab: Dict[str, BaseVocab], char_emb_dim: int,\n",
    "                 char_hidden_dim: int, char_num_layers: int, dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        ### Task 2.1 starts here ###\n",
    "        self.char_emb = ...\n",
    "        ### Task 2.1 ends here ###\n",
    "\n",
    "        ### Task 2.2 starts here ###\n",
    "        self.char_lstm = ...\n",
    "        self.char_lstm_h_init = nn.Parameter(torch.zeros(char_num_layers, 1, char_hidden_dim))\n",
    "        self.char_lstm_c_init = nn.Parameter(torch.zeros(char_num_layers, 1, char_hidden_dim))\n",
    "        ### Task 2.2 ends here ###\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, chars_pad, sent_lens, word_lens):\n",
    "        ### Task 2.3 starts here ###\n",
    "        char_emb = ...\n",
    "        ### Task 2.3 ends here ###\n",
    "\n",
    "        batch_size = char_emb.size(0)\n",
    "        char_emb = pack_padded_sequence(char_emb, word_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        ### Task 2.4 starts here ###\n",
    "        _, (h, _) = self.char_lstm(\n",
    "            ...,\n",
    "            (self.char_lstm_h_init.expand(char_num_layers, batch_size, char_hidden_dim).contiguous(),\n",
    "             self.char_lstm_c_init.expand(char_num_layers, batch_size, char_hidden_dim).contiguous())\n",
    "        )\n",
    "        ### Task 2.4 ends here ###\n",
    "\n",
    "        # Remove an empty dimension\n",
    "        result = h.squeeze(0)\n",
    "        # Chunk the output back into words\n",
    "        result = pack_sequence(result.split(sent_lens), enforce_sorted=False)\n",
    "\n",
    "        return result"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXaj5bXEQxAw"
   },
   "source": [
    "### Task 2.5. Incorporate the CharLSTM into the Tagger model (0.5 points)\n",
    "\n",
    "Now your new model should have the following architecture:\n",
    "\n",
    "![img](https://github.com/501Good/tartu-nlp-2020/blob/master/homeworks/hw4/img1.png?raw=1)\n",
    "\n",
    "Initialize the CharLSTM model that you've just created. Pass `vocab`, `char_emb_dim`, `char_hidden_dim`, `char_num_layers`, `dropout` to the class constructor. Save it to `self.char_model`.\n",
    "\n",
    "Create another [`nn.Linear`](https://pytorch.org/docs/stable/nn.html?highlight=linear#torch.nn.Linear) layer to transform the character representations. It should work similar to the linear layer for the transformation of the pretrained vectors. The input size should be `char_hidden_dim` and the output size is `transformed_dim`. Also, set the `bias` parameter to `False`.\n",
    "\n",
    "Add the `transformed_dim` to the `input_size`.\n",
    "\n",
    "### Task 2.6. Get the character embeddings in the forward pass (0.5 points)\n",
    "\n",
    "Perform the forward pass on your CharLSTM and save the results into the `char_emb` variable. Pass all the appropriate parameters to the CharLSTM.\n",
    "\n",
    "Apply the dropout to the `char_emb`. After that transform it with the `self.trans_char` linear layer. \n",
    "\n",
    "**NB!** Since the output of the CharLSTM is a `PackedSequence` object, you need to apply the dropout to `char_emb.data`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-N-xVFW28FD3"
   },
   "source": [
    "class Tagger(nn.Module):\n",
    "    def __init__(self, vocab: Dict[str, BaseVocab], word_emb_dim: int,\n",
    "                 char_emb_dim: int, transformed_dim: int, emb_matrix: np.ndarray,\n",
    "                 hidden_dim: int, char_hidden_dim: int,\n",
    "                 upos_clf_hidden_dim: int, num_layers: int, char_num_layers: int,\n",
    "                 dropout: float):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab = vocab\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        input_size = 0\n",
    "\n",
    "        self.word_emb = nn.Embedding(len(vocab['word']), word_emb_dim, padding_idx=0)\n",
    "        input_size += word_emb_dim\n",
    "\n",
    "        ### Task 2.5 starts here ###\n",
    "        self.char_model = ...\n",
    "        self.trans_char = ...\n",
    "        input_size += transformed_dim\n",
    "        ### Task 2.5 ends here ###\n",
    "\n",
    "        self.pretrained_emb = nn.Embedding.from_pretrained(torch.from_numpy(emb_matrix), freeze=True)\n",
    "        self.trans_pretrained = nn.Linear(emb_matrix.shape[1], transformed_dim, bias=False)\n",
    "        input_size += transformed_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.lstm_h_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
    "        self.lstm_c_init = nn.Parameter(torch.zeros(2 * num_layers, 1, hidden_dim))\n",
    "\n",
    "        self.upos_hid = nn.Linear(2* hidden_dim, upos_clf_hidden_dim)\n",
    "        self.upos_clf = nn.Linear(upos_clf_hidden_dim, len(vocab['upos']))\n",
    "\n",
    "        self.crit = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    \n",
    "    def forward(self, sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens):\n",
    "        inputs = []\n",
    "\n",
    "        word_emb = self.word_emb(sent_pad)\n",
    "        inputs += [word_emb]\n",
    "\n",
    "        pretrained_emb = self.pretrained_emb(pretrained_pad)\n",
    "        pretrained_emb = self.trans_pretrained(pretrained_emb)\n",
    "        inputs += [pretrained_emb]\n",
    "\n",
    "        ### Task 2.6 starts here ###\n",
    "        char_emb = ...\n",
    "        char_emb_trans = ...\n",
    "        ### Task 2.6 ends here ###\n",
    "        # Creating a PackedSequence from the embeddings to restore the original order\n",
    "        char_emb = PackedSequence(char_emb_trans, char_emb.batch_sizes, \n",
    "                                  char_emb.sorted_indices, char_emb.unsorted_indices)\n",
    "        char_emb = pad_packed_sequence(char_emb, batch_first=True)[0]\n",
    "        inputs += [char_emb]\n",
    "\n",
    "        lstm_inputs = torch.cat([x for x in inputs], 2)\n",
    "        lstm_inputs = self.drop(lstm_inputs)\n",
    "        lstm_inputs = pack_padded_sequence(lstm_inputs, sent_lens, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        lstm_outputs, _ = self.lstm(\n",
    "            lstm_inputs, \n",
    "            (self.lstm_h_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous(), \n",
    "             self.lstm_c_init.expand(2 * self.num_layers, sent_pad.size(0), self.hidden_dim).contiguous())\n",
    "        )\n",
    "        lstm_outputs = lstm_outputs.data\n",
    "\n",
    "        upos_hid = F.relu(self.upos_hid(self.drop(lstm_outputs)))\n",
    "        upos_pred = self.upos_clf(self.drop(upos_hid))\n",
    "\n",
    "        pred = PackedSequence(upos_pred, lstm_inputs.batch_sizes,\n",
    "                              lstm_inputs.sorted_indices, lstm_inputs.unsorted_indices)\n",
    "        pred = pad_packed_sequence(pred, batch_first=True)[0]\n",
    "        pred = pred.max(2)[1]\n",
    "        upos = pack_padded_sequence(upos_pad, sent_lens, batch_first=True, enforce_sorted=False).data\n",
    "        loss = self.crit(upos_pred, upos)\n",
    "\n",
    "        return loss, pred"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9-iZlzjyF4no"
   },
   "source": [
    "class Trainer:\n",
    "    def __init__(self, vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
    "                 emb_matrix, hidden_dim, char_hidden_dim, upos_clf_hidden_dim, \n",
    "                 num_layers, char_num_layers, dropout):\n",
    "        self.vocab = vocab\n",
    "        self.model = Tagger(vocab, word_emb_dim, char_emb_dim, transformed_dim, \n",
    "                            emb_matrix, hidden_dim, char_hidden_dim, \n",
    "                            upos_clf_hidden_dim, num_layers, char_num_layers, \n",
    "                            dropout)\n",
    "        self.parameters = [p for p in self.model.parameters() if p.requires_grad]\n",
    "\n",
    "        self.model.to(device)\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.parameters)\n",
    " \n",
    "    def update(self, batch, eval=False):\n",
    "        sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens = batch\n",
    "\n",
    "        if eval:\n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "        loss, _ = self.model(sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens)\n",
    "        loss_val = loss.data.item()\n",
    "        if eval:\n",
    "            return loss_val\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss_val\n",
    "\n",
    "\n",
    "    def predict(self, batch):\n",
    "        sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens = batch\n",
    "\n",
    "        self.model.eval()\n",
    "        batch_size = sent_pad.size(0)\n",
    "        _, pred = self.model(sent_pad, chars_pad, upos_pad, pretrained_pad, sent_lens, word_lens)\n",
    "        # Transform the indices to the pos tags\n",
    "        pred = [self.vocab['upos'].unmap(sent) for sent in pred.tolist()]\n",
    "        # Trim the predictions to their original lengths\n",
    "        pred_tokens = [[pred[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
    "\n",
    "        gold_upos = [vocab['upos'].unmap(upos) for upos in [upos for upos in upos_pad]]\n",
    "        gold_tokens = [[gold_upos[i][j] for j in range(sent_lens[i])] for i in range(batch_size)]\n",
    "\n",
    "        return pred_tokens, gold_tokens"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gAu5ZF94JMvN"
   },
   "source": [
    "word_emb_dim = 75\n",
    "char_emb_dim = 100\n",
    "transformed_dim = 125\n",
    "emb_matrix = pretrain.emb\n",
    "hidden_dim = 200\n",
    "char_hidden_dim = 400\n",
    "upos_clf_hidden_dim = 400\n",
    "num_layers = 2\n",
    "char_num_layers = 1\n",
    "dropout = 0.5"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kiKLCdBaJG-t"
   },
   "source": [
    "trainer = Trainer(vocab, word_emb_dim, char_emb_dim, transformed_dim,\n",
    "                  emb_matrix, hidden_dim, char_hidden_dim, upos_clf_hidden_dim,\n",
    "                  num_layers, char_num_layers, dropout)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KjJm834cJ-kZ"
   },
   "source": [
    "global_step = 0\n",
    "max_steps = 50000\n",
    "dev_score_history = []\n",
    "format_str = '{}: step {}/{}, loss = {:.6f} ({:.3f} sec/batch)'\n",
    "last_best_step = 0\n",
    "\n",
    "log_step = 20\n",
    "eval_interval = 100"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rT2AjVnth0_L"
   },
   "source": [
    "## Task 3. Train your model (0.5 points)\n",
    "\n",
    "Run the cell below to start training the model. After each 100 steps, it is going to print out the average training loss and dev score, which is a simple accuracy in our case. You should see the training loss decreasing and the dev score increasing.\n",
    "\n",
    "Train the model until you don't see the increase in the dev score anymore. Report the score that you've got.\n",
    "\n",
    "__My maximum dev_score is:__\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KY5M8lS5Kibm"
   },
   "source": [
    "train_loss = 0\n",
    "while True:\n",
    "    do_break = False\n",
    "    for batch in train_loader:\n",
    "        start_time = time()\n",
    "        global_step += 1\n",
    "        loss = trainer.update(batch, eval=False)\n",
    "        train_loss += loss\n",
    "\n",
    "        if global_step % log_step == 0:\n",
    "            duration = time() - start_time\n",
    "            print(format_str.format(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"), global_step,\\\n",
    "                    max_steps, loss, duration))\n",
    "            \n",
    "        if global_step % eval_interval == 0:\n",
    "            print(\"Evaluating on dev set...\")\n",
    "            dev_preds = []\n",
    "            dev_words = []\n",
    "            dev_correct = 0\n",
    "            dev_total = 0\n",
    "            for batch in dev_loader:\n",
    "                batch_size = batch[0].size(0)\n",
    "                preds, gold = trainer.predict(batch)\n",
    "                dev_correct += sum([1 for sent in zip(preds, gold) for pair in zip(*sent) if pair[0] == pair[1]])\n",
    "                dev_total += sum([len(sent) for sent in gold])\n",
    "                dev_preds += preds\n",
    "                # Keep the original sentence\n",
    "                pred_sents = [[batch[0][i][j] for j in range(batch[4][i])] for i in range(batch_size)]\n",
    "                dev_words += [vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]]\n",
    "            \n",
    "            dev_score = dev_correct / dev_total\n",
    "            train_loss = train_loss / eval_interval\n",
    "            print(\"step {}: train_loss = {:.6f}, dev_score = {:.6f}\".format(global_step, train_loss, dev_score))\n",
    "            # Shows one prediction for a sanity check\n",
    "            print(f\"Preds: {list(zip(dev_preds[0], dev_words[0]))}\")\n",
    "            train_loss = 0\n",
    "\n",
    "        if global_step >= max_steps:\n",
    "            do_break = True\n",
    "            break\n",
    "\n",
    "        if do_break:\n",
    "            break"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4bjvrTcQYKHS"
   },
   "source": [
    "### Task 4. Error Analysis (1 point)\n",
    "\n",
    "Let's evaluate the model on the test set. First you need to load in the testing data. Create test_doc, test_dataset and test_loader. Evaluate the trainer on the test set.\n",
    "\n",
    "\n",
    "Create a confusion matrix, display it in readable format.\n",
    "Now, when we have the confusion matrix, let's calculate accuracy for each POS tag separately. \n",
    "\n",
    "Lastly, look at the confusion matrix and accuracy for each POS tag and describe what issues you can see. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5_znTbidq-3o"
   },
   "source": [
    "test_doc = ...\n",
    "test_dataset = ...\n",
    "test_loader = ...."
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3y7u9tl0rO6S"
   },
   "source": [
    "test_preds = []\n",
    "test_words = []\n",
    "test_golds = []\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "for batch in test_loader:\n",
    "    batch_size = batch[0].size(0)\n",
    "    preds, gold = trainer.predict(batch)\n",
    "    test_golds += gold\n",
    "    test_correct += sum([1 for sent in zip(preds, gold) for pair in zip(*sent) if pair[0] == pair[1]])\n",
    "    test_total += sum([len(sent) for sent in gold])\n",
    "    test_preds += preds\n",
    "    pred_sents = [[batch[0][i][j] for j in range(batch[4][i])] for i in range(batch_size)]\n",
    "    test_words += [vocab['word'].unmap(sent) for sent in [sent for sent in pred_sents]]\n",
    "test_score = test_correct / test_total\n",
    "print(test_score)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "JR82zLpiBc6H"
   },
   "source": [
    "# Create the confusion matrix, you can use the sklearn confusion matrix \n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dqjzSnQbYJi8"
   },
   "source": [
    "# Calculate accuracy for each POS tag separately using confusion matrix\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAexkx8mF8Wq"
   },
   "source": [
    "__What were the issues that you can see from the confusion matrix and calculated accuracies?__\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jIVnti9di2k9"
   },
   "source": [
    "## Task 5. Propose a new approach (2 points)\n",
    "\n",
    "So far, you should have an idea on how the sequence tagging is done. Now you will have to propose an approach for a new problem.\n",
    "\n",
    "Imagine that you need to add morphological tagging to your model. The tags are stored in the [_feats_ field](https://universaldependencies.org/format.html#morphological-annotation) of the CoNLL-U format. From the official description, thay have the following format:\n",
    "\n",
    "> The FEATS field contains a list of morphological features, with vertical bar (|) as list separator and with underscore to represent the empty list. All features should be represented as attribute-value pairs, with an equals sign (=) separating the attribute from the value.\n",
    "\n",
    "Your task is to describe how you will modify the current POS tagger model to include FEATS tagger as well. Your description must answer the following questions:\n",
    "\n",
    "- Should you add a new vocab to read the feats? How will you build this vocab?\n",
    "- POS tagging can be treated as a multi-class classification task, i.e. we assign each word to exaclty one POS tag (which is a class in our situation). What kind of task is morphological tagging (multi-label, binary classification, multiple multi-class tasks)?\n",
    "- For example, nouns have case and gender attributes but verbs don't have them. How are you going to tackle this?\n",
    "- Which new layers are you going to introduce into the model?\n",
    "- What metrics are you going to use to measure the performance of morphological tagging?\n",
    "\n",
    "You can also try to visualize the model to make it easier to see your concept.\n",
    "\n",
    "<font color='red'>Your answer here</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YcGoPgJLYO_U"
   },
   "source": [
    "### Bonus. Hidden Markov Model for POS tagging (1 point)\n",
    "\n",
    "Hidden Markov Model (HMM) is a probabilistic model, for POS tagging it will take as an input a sequence of words, computes a possible sequences of POS tags from it and then chooses the best sequence of tags. \n",
    "\n",
    "The POS tags cannot be observed directly (they are hidden), we only see the words and infer the tags from the sequence. \n",
    "\n",
    "HMM is defined with the following components: \n",
    "\n",
    "$ Q = Q_1q_2...q_N $ - a set of N states <br>\n",
    "$ A = a_{11}...a_{ij}...a_{NN}$ - transition probability matrix, each $a_{ij}$ represents the probability of moving from state i to state j.  <br>\n",
    "$O=o_1o_2...o_T$ - a sequence of T observations <br>\n",
    "$B=b_i(o_t)$ - sequence of observation likelihoods (emission probabilities), each expresses the probability of an observation $o_t$ being generated from a state $q_i$  <br>\n",
    "$\\pi=\\pi_1,\\pi_2,...,\\pi_N$ - initial probability distribution over states. $+\\pi_iÂ£ is the probability that the Markov chain will start in state i. \n",
    "\n",
    "We also need to know the two simplifying assumptions:\n",
    "\n",
    "1) The probability of a particular state depends only on the previous state: <br>\n",
    " Makrov Assumption: $$P(q_i|q_1,...q_{i-1})=P(q_i|q_{i-1})$$\n",
    "\n",
    "2) The probability of an output observation $o_i$ depends only on the state that produced the observation $q_i$ and not on any other states or any other observations: <br>\n",
    "Output Independence: $$P(o_i|q_1,...q_i,...q_T,o_1,...o_i,...o_T)=P(o_i|q_i)$$\n",
    "\n",
    "\n",
    "HMM itself has two components: transition probabilities and emission probabilities. \n",
    "\n",
    "We can calculate the maximum likelihood estimate for transition probability by counting out of the times we see the first tag in a labeled corpus, how often the first tag is followed by the second. <br>\n",
    "\n",
    "$$P(t_i|t_{i-1}) = \\frac{C(t_{i-1}, t_i)}{C(t_{i-1})}$$\n",
    "\n",
    "\n",
    "We can calculate the emission probabilities by counting how many times we see the tag in the corpus and how many times this tag is assigned to a specific word:\n",
    "\n",
    "$$P(w_i|t_i) = \\frac{C(t_i, w_i)}{C(t_i)}  $$\n",
    "\n",
    "You can read more about the HMM from the Dainel Jurafsky & James H. Martin [book](https://web.stanford.edu/~jurafsky/slp3/8.pdf) called Speech and Language Processing. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "unozGBA9MHaT"
   },
   "source": [
    "import nltk \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import random\n",
    "!pip install conllu\n",
    "from conllu import parse"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5ARgo8F0qc9"
   },
   "source": [
    "Read in the UD data that you have already downloaded earlier. Replace the path to train and test dataset with your own downloaded splits. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vG92XBVpnqe8"
   },
   "source": [
    "train_path = Path('data') / ... / ...\n",
    "test_path = Path('data') / ... / ... \n",
    "\n",
    "train_data = open(train_path, 'r', encoding='utf-8').read()\n",
    "train_sentences = parse(train_data)\n",
    "train_set = [[(token.get('form'),token.get('upos')) for token in sentence] for sentence in train_sentences ]\n",
    "\n",
    "test_data = open(test_path, 'r', encoding='utf-8').read()\n",
    "test_sentences = parse(test_data)\n",
    "test_set = [[(token.get('form'),token.get('upos')) for token in sentence ] for sentence in test_sentences]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2UEny_HPot7X"
   },
   "source": [
    "train_tagged = [ pair for pair in sent for sent in train_set ]\n",
    "test_tagged = [ pair for pair in sent for sent in test_set ]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RYTBlUZ8pOmz"
   },
   "source": [
    "pos_tags = list({tag for _,tag in train_tagged})\n",
    "print(pos_tags)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUry7eGv1ul-"
   },
   "source": [
    "Now, we have the tagged sentences, the POS tags and vocabulary. \n",
    "Next, we need to calculate emission and transition probabilities. Use the formulas given above. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P-GMzFDYmlyG"
   },
   "source": [
    "def emission_probability(word, tag, train=train_tagged):\n",
    "  c_t1 = ... # count how many times this tag occurs in the dataset\n",
    "  c_tiwi = ... # count how many times this tag has been associated with the given word\n",
    "  p_witi = ... # calculate the emission probability \n",
    "  return p_witi"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TFY_xkowmoaF"
   },
   "source": [
    "def transition_probability(tag1, tag2, train = train_tagged):\n",
    "    tags = ... # collect all the pos tags from the train\n",
    "    c_ti = ... # count all the times we see tag1 in the corpus \n",
    "    c_ti_ti1 = 0\n",
    "    for index in range(len(tags)-1):\n",
    "        if ...:  # count the times we see tag1 before tag2\n",
    "            c_ti_ti1  += 1\n",
    "    p_titi1 = ... # calculate the transition_probability\n",
    "    return p_titi1"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHdcSlHB7Jkp"
   },
   "source": [
    "Now, let's create a transition matrix: "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YO0lBPInmtYk"
   },
   "source": [
    "transition_matrix = np.zeros((len(pos_tags), len(pos_tags)), dtype='float32')\n",
    "for i, tag1 in enumerate(list(pos_tags)):\n",
    "    for j, tag2 in enumerate(list(pos_tags)): \n",
    "        transition_matrix[i, j] = ... # assign the correct transition probability (hint: call the transition_probability function)\n",
    " \n",
    "df_tags = pd.DataFrame(transition_matrix, columns=list(pos_tags), index=list(pos_tags))\n",
    "display(df_tags)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fqwTjHod2c0T"
   },
   "source": [
    "To decode the sequence from HMM we can use Viterbi algorithm, which is a dynamical programming algorithm. For POS tagging, we need to find the most probable tag sequence given the observation sequence of n words. \n",
    "\n",
    "There are two assumptions made while decoding tag sequence: \n",
    "\n",
    "1) The probability of a word appearing depends only on its own tag and is independent of neighboring words and tags. \n",
    "\n",
    "2) The probability of a tag depends only on the previous tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUJ7qSUIjbN_"
   },
   "source": [
    "<img src=\"https://miro.medium.com/max/540/1*8-5KZVj-_jZOWN83gGhD5A.png\" >\n",
    "Image from Medium post *POS Tagging using Hidden Markov Models (HMM) & Viterbi algorithm in NLP mathematics explained* by Mehul Gupta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mX9pQpZmj5Rl"
   },
   "source": [
    "First, we need to define probability matrix called lattice (let's define it as V). Each row represents a POS tag (hidden state) and each column represents a word (observables). Look at the image above to get an idea of how this matrix looks. We need to fill in this matrix by calculating the probabilities for each cell, for example for $V_{jt}$, we have to calculate the probability that the HMM is in some state j after seeing the first t observations. The $V_{jt}$ can be calculated: \n",
    "$$V_{jt}= max(V_{t-1}*a[i,j]*b_j(o_t))$$\n",
    "Here, $v_{t-1}(i)$ is the previous Viterbi path probability from the previous time step. $a_{ij}$ is the transition probability from previous state $q_i$ to current state $q_j$. $b_j(o_t)$ is the state observation likelihood of the observation symbol $o_t$ given the current state j. \n",
    "\n",
    "\n",
    "Your task regarding the Viterbi algorithm is to just fill in the slots in the function below. You need to take the correct transition probability given the previous tag and current tag from the df_tags matrix. You can access the previous tag from the states list. \n",
    "\n",
    "Next, you need to calculate the emission probability given the word and the tag. \n",
    "\n",
    "After all the probabilities for each POS for the word have been calculated, we will be selecting the maximum probability from the probabilities list and add the best state (POS tag) to the states list. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TMl6oq34mzAL"
   },
   "source": [
    "def Viterbi(words,tags, transition_matrix=df_tags):\n",
    "    states = [] # we are saving the best states only \n",
    "    for key, word in enumerate(words): # iterating over the observations\n",
    "        probabilities = [] # saving the probabilities \n",
    "        for tag in tags: # iterating over the observations (we are filling one column)\n",
    "            if key == 0: # if the word is the first one in the sequence\n",
    "                transition_prob = df_tags.loc['PUNCT', tag]\n",
    "            else:\n",
    "                transition_prob = ... \n",
    "            \n",
    "            emission_prob = ...\n",
    "            state_prob = emission_prob * transition_prob \n",
    "            probabilities.append(state_prob)\n",
    "             \n",
    "        pmax = ... # take the maximum value from the probabilities list\n",
    "        state_max = ... # get the state so the probability is maximum (hint: get the index where this maximum probability was in the probabilities list and use it to get the tag from the tags list)\n",
    "        states.append(state_max)\n",
    "    return list(zip(words, states))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTlvbQvwuvRC"
   },
   "source": [
    "Let's test the algorithm. Let's choose 15 sentences from the test set and calculate the accuracy on this set. If you want you can choose bigger sample. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nni672gfnc6v"
   },
   "source": [
    "random_set = [random.randint(1,len(test_set)-1) for x in range(15)]\n",
    " \n",
    "test = [test_set[i] for i in random_set]\n",
    "test_tags = [tup[1] for sent in test for tup in sent]\n",
    "test_words = [tup[0] for sent in test for tup in sent]\n",
    "tagged_seq  = Viterbi(test_words, list(pos_tags))\n",
    "\n",
    "check = [pred  for pred, gold in zip(tagged_seq, test_tags) if pred[1] == gold] \n",
    " \n",
    "accuracy = len(check)/len(tagged_seq)\n",
    "print('Accuracy for a small subset: ',accuracy*100)"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}