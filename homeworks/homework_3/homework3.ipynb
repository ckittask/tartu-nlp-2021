{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "homework3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1bYLpPHc2Ri"
      },
      "source": [
        "# Homework 3: Word Embeddings\r\n",
        "\r\n",
        "Welcome to homework 3! \r\n",
        "\r\n",
        "The homework contains several tasks. You can find the amount of points you get for the correct solution in the task header. Maximum amount of points for each homework is 7 + 1 (bonus exercise). \r\n",
        "The **grading** for each task is the following: \r\n",
        "* correct answer - **full points** \r\n",
        "* insufficient solution or solution resulting in the incorrect output - **half points**\r\n",
        "* no answer or completely wrong solution - **no points**\r\n",
        "\r\n",
        "Even if you don't know how to solve the task, we encourage you to write down your thoughts and progress and try to address the issues that stop you from completing the task.\r\n",
        "\r\n",
        "When working on the written tasks, try to make your answers short and accurate. Most of the times, it is possible to answer the question in 1-3 sentences.\r\n",
        "\r\n",
        "When writing code, make it readable. Choose appropriate names for your variables (a = 'cat' - not good, word = 'cat' - good). Avoid constructing lines of code longer than 100 characters (79 characters is ideal). If needed, provide the commentaries for your code, however, a good code should be easily readable without them :)\r\n",
        "\r\n",
        "Finally, all your answers should be written only by yourself. If you copy them from other sources it will be considered as an academic fraud. You can discuss the tasks with your classmates but each solution must be individual.\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "**Before sending your solution, do the Kernel -> Restart & Run All to ensure that all your code works.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhgPZ8Vfh3YK"
      },
      "source": [
        "## More than a word\r\n",
        "So far, we studied how we can represent separate words as dense vectors, also called embeddings. But what if we need to find similar documents? How do we represent a whole document, that can have thousands of words as a vector?\r\n",
        "\r\n",
        "In this homework, you will learn and try different techniques of document representation.\r\n",
        "\r\n",
        "But first, let's start with imports. In this homework, you are going to use [Gensim](https://radimrehurek.com/gensim/) library. The installation should not cause any troubles. However, **install Cython before installing Gensim!** This will allow Gensim to use efficient calculations and reduce the training time from several minutes to several seconds. Thus, the installation procedure should be the following:\r\n",
        "\r\n",
        "pip install Cython\r\n",
        "\r\n",
        "pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3IpAPSObcqtc"
      },
      "source": [
        "import csv\r\n",
        "from pathlib import Path\r\n",
        "from collections import namedtuple, Counter\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\r\n",
        "\r\n",
        "import spacy\r\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlYoHDt9Yd0p"
      },
      "source": [
        "## Task 0: Data\r\n",
        "\r\n",
        "Load the data and get familiar with its structure. We provided all the functions for loading the dataset.\r\n",
        "\r\n",
        "The data is a compilation of movie plot synopses from IMDb and Wikipedia. You can find the original data here, as well as the paper describing it: Kar, S., Maharjan, S., LÃ³pez-Monroy, A. P., & Solorio, T. (2018). MPST: A corpus of movie plot synopses with tags. arXiv preprint arXiv:1802.07858..\r\n",
        "\r\n",
        "The data was modified as follows: only title, plot_synopsis, tags, and split colums were left. All the synopses were tokenize, lemmatized, cleared from the stop words with Spacy. Final data was reduced to 1000 training samples and 500 test samples.\r\n",
        "\r\n",
        "Run the cells below to load the data and see an example of the first training sample:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0dochrGaUaA"
      },
      "source": [
        "MovieInfo = namedtuple('MovieInfo', ['title', 'synopsis_preprocessed', 'synopsis_original', 'tags'])\r\n",
        "\r\n",
        "data_path = Path('movie_plots.csv')\r\n",
        "\r\n",
        "train_set, test_set = [], []\r\n",
        "\r\n",
        "with data_path.open(encoding='utf-8', newline='') as csvfile:\r\n",
        "    reader = csv.DictReader(csvfile)\r\n",
        "    for row in reader:\r\n",
        "\r\n",
        "        tags = [tag.strip() for tag in row['tags'].split(',')]\r\n",
        "        movie_info = MovieInfo(\r\n",
        "            row['title'], row['synopsis_preprocessed'], row['synopsis_original'], tags\r\n",
        "        )\r\n",
        "        if row['split'] == 'train':\r\n",
        "            train_set.append(movie_info)\r\n",
        "        elif row['split'] == 'test':\r\n",
        "            test_set.append(movie_info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5B0mfV2Yncd"
      },
      "source": [
        "print(f\"Train set has {len(train_set)} entries\")\r\n",
        "print(f\"Test set has {len(test_set)} entries\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scHF-liqYoPp"
      },
      "source": [
        "print(\"Movie info example\")\r\n",
        "print(\"-\" * 50)\r\n",
        "print(\"Title:\")\r\n",
        "print(train_set[0].title)\r\n",
        "print(\"-\" * 50)\r\n",
        "print(\"Preprocessed synopsis:\")\r\n",
        "print(train_set[0].synopsis_preprocessed)\r\n",
        "print(\"-\" * 50)\r\n",
        "print(\"Original synopsis:\")\r\n",
        "print(train_set[0].synopsis_original)\r\n",
        "print(\"-\" * 50)\r\n",
        "print(\"Tags:\")\r\n",
        "print(train_set[0].tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDGkPyqHYrEu"
      },
      "source": [
        "## Task 1: TF-IDF (5 points)\r\n",
        "\r\n",
        "TF-IDF is one of the techniques to represent a text as a vector. TF-IDF means Term Frequency - Inversed Document Frequency. The hyphen between two term does not mean substraction, the terms are multiplies with each other.\r\n",
        "\r\n",
        "For this approach, we are going to create a document matrix, that is going to have a TF-IDF value for each word in every document. For each document, this value is going to be different.\r\n",
        "\r\n",
        "Let's dive into this gradually, starting with the first part - term frequency.\r\n",
        "\r\n",
        "As you might've guesses from the name, term frequency shows us how many times each word appears in a document. But just a simple counter is going to prioritize very frequent words, like is or me, so we are going to normalize it. It gives us the following formula for the term frequency:\r\n",
        "\r\n",
        "tf(t, d) = count of t in d / number of words in d\r\n",
        "\r\n",
        "where t is the term, or a word, and d is a document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7Tw9lkKYw4W"
      },
      "source": [
        "### Task 1.1. Build a vocab (0.5 points)\r\n",
        "Before we start let's build a vocabulary, containing all the unique words in our training set.\r\n",
        "\r\n",
        "Complete the build_vocab() function, so it takes a list of texts, each of them represented by a list of tokens (texts = [['token1', 'token2', ... 'token_i'], ['token1', 'token2', ... 'token_i'], ..., ['token1', 'token2', ... 'token_i']]) and returns a dict with a word as a key and its index as a number (vocab = {'and': 0, 'me': 1, ..., 'zimbabwe': 546})."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzTlicW8Y6Yk"
      },
      "source": [
        "def build_vocab(texts):\r\n",
        "    ...\r\n",
        "    return vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f3tUmWqqY78t"
      },
      "source": [
        "def get_preprocessed_synopses(data_set):\r\n",
        "    return [entry.synopsis_preprocessed.split() for entry in data_set]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKFOpTQJY_Zm"
      },
      "source": [
        "vocab = build_vocab(get_preprocessed_synopses(train_set))\r\n",
        "print(\"Total length of the vocab:\", len(vocab))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ib0uYf8RZC3Z"
      },
      "source": [
        "### Task 1.2. Count TF (1 point)\r\n",
        "With the vocabulary built, construct a term frequency matrix. This matrix should have a shape of (number of documents, size of vocab) and contain a count for each term. For example, is a word cat is seen 12 times in the document #100, you will have tf[100, vocab['cat']] = 12.\r\n",
        "\r\n",
        "Complete the tf() function below and make sure you get the correct output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SodLkJrlZE2Z"
      },
      "source": [
        "def tf(texts, vocab):\r\n",
        "    # Store the TF value here\r\n",
        "    tf = np.zeros((len(texts), len(vocab)))\r\n",
        "    \r\n",
        "    # Store the length of each document here\r\n",
        "    doc_lens = np.zeros(len(texts))\r\n",
        "    \r\n",
        "    ...\r\n",
        "    \r\n",
        "    tf = tf / doc_lens[:, np.newaxis]\r\n",
        "    return tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ef2ytWMUZH27"
      },
      "source": [
        "train_tf = tf(get_preprocessed_synopses(train_set), vocab)\r\n",
        "\r\n",
        "print(\"Shape of the train TF matrix should be (1000, 33537):\", train_tf.shape)\r\n",
        "print(\"Sum of all TFs for the first synopsis should be 1:\", np.sum(train_tf[0, :]))\r\n",
        "print()\r\n",
        "\r\n",
        "test_tf = tf(get_preprocessed_synopses(test_set), vocab)\r\n",
        "\r\n",
        "print(\"Shape of the train TF matrix should be (500, 33537):\", test_tf.shape)\r\n",
        "print(\"Sum of all TFs for the first synopsis should be 1:\", np.sum(test_tf[0, :]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8fWW-_GZI9r"
      },
      "source": [
        "### Task 1.3. Normalize your TF (0.5 points)\r\n",
        "Good news is that we can already find similar documents with only our TF matrix! We can do it by calculating a cosine similarity between the vectors of two documents. In our case, the vectors for the documents are the rows of the TF matrix that contain the frequency value for each term. But as we learned with word embeddings, to make the cosine value be relevant, we need to normalize all the vectors, i.e. make the all of length 1.\r\n",
        "\r\n",
        "To normalize a vector, we need to divide each value by the length of the vector. Please, use the np.linalg.norm function to find the length of the vector. Read the documentation and choose the correct axis value.\r\n",
        "\r\n",
        "When dividing the tf matrix by our lengths vector, you might want to add [:, np.newaxis] to avoid shape mismatching errors (see the tf() function).\r\n",
        "\r\n",
        "Complete the normalize() function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LH4g5kViZNEc"
      },
      "source": [
        "def normalize(tf):\r\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xytscm9ZOHM"
      },
      "source": [
        "test_tf_norm = normalize(test_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AlfrTOsZQS2"
      },
      "source": [
        "### Task 1.4. Cosine similarity (0.5 points)\r\n",
        "Finish the cosine_similarity() function. It should take one vector as a query argument, the matrix of vectors for each document as a pool argument, and the number of the most similar documents to return as a k argument.\r\n",
        "\r\n",
        "For each document in the pool we caclulate a cosine similarity with the query, which is a dot product of the corresponding two vectors.\r\n",
        "\r\n",
        "The return value of the function should the a numpy array of length k, containing the indices for the most similar documents sorted in descending order, i.e. from the most similar document to the least similar one.\r\n",
        "\r\n",
        "In this fucntion you might want to use np.argsort and np.dot functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iSdkvCXZTSS"
      },
      "source": [
        "def cosine_similarity(query, pool, k=10):\r\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDIt7WwWZWze"
      },
      "source": [
        "Sanity check of our cosine_similarity() function. We check the most similar documents to the first document in our training set. Obviously, the same document should be the most similar.\r\n",
        "\r\n",
        "The output should be something like this: array([  0, 177, 187, 344, 407, 518, 766, 861, 255, 316], dtype=int64).\r\n",
        "\r\n",
        "If the first element is not 0, it means that you did something wrong during the previous steps!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FROiq9p6ZX9c"
      },
      "source": [
        "cosine_similarity(train_tf_norm[0, :], train_tf_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCMyzKCoZa7P"
      },
      "source": [
        "We are also going to introduce a very naive and simple method to evaluate the quality of our predictions. For each predicted text, we are going to look at its tags and mark it as correct if at least one tag in the query and prediction is the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Deq3DsPyZdkF"
      },
      "source": [
        "def accuracy(pred, gold):\r\n",
        "    assert len(pred) == len(gold)\r\n",
        "    correct = 0\r\n",
        "    total = 0\r\n",
        "    for i in range(len(pred)):\r\n",
        "        total += 1\r\n",
        "        if any(x in pred[i] for x in gold[i]):\r\n",
        "            correct += 1\r\n",
        "    return correct / total"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnAFxG_KZeY3"
      },
      "source": [
        "Let's see the accuracy for our TF classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFimotVmZgHm"
      },
      "source": [
        "tf_pred = []\r\n",
        "gold = []\r\n",
        "for i, test_entry in enumerate(test_tf_norm):\r\n",
        "    most_similar = cosine_similarity(test_entry, train_tf_norm, k=1)\r\n",
        "    gold.append(test_set[i].tags)\r\n",
        "    tf_pred.append(train_set[most_similar[0]].tags)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ltSIBdaZh86"
      },
      "source": [
        "print(accuracy(tf_pred, gold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoU9ORHkZk4E"
      },
      "source": [
        "### Task 1.5. Count DF (1 point)\r\n",
        "Let's proceed to the second part of the TF-IDF which is inversed document frequency. But before we inverse something, let's find the document frequency first. We define the document frequency as df(t) = occurrence of t in documents.\r\n",
        "\r\n",
        "To find it, we can use a simple but very slow approach of iterating through each document and comparing each word in it against the vocabulary. There is an alternative way to do it really fast, only using the tf matrix that we already constructed above.\r\n",
        "\r\n",
        "The df() function should return an array of length of our vocab, where for each respective word index, we have a value representing the number of documents with word was seen in.\r\n",
        "\r\n",
        "Try to find a df array with one-liner. Hint: You may want to use np.sum() function. Also, remember that you can use equality operators (<, >, ==) with numpy arrays.\r\n",
        "\r\n",
        "The correct one-line solution will get 1 point, the slow iterative solution with only get 0.5 points!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hVI75cAZoFW"
      },
      "source": [
        "def df(tf):\r\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WqDYOuwZsx_"
      },
      "source": [
        "train_df = df(train_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf3G9cbPZveF"
      },
      "source": [
        "Your train_df array should look like this: array([25,  2,  6, ...,  3,  1, 31]). You will probably have different numbers in the array since the initialization of the dictionary is different on your machine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxNGuS7nZzMc"
      },
      "source": [
        "train_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XcBhPrnkZ332"
      },
      "source": [
        "### Task 1.6. Count IDF (0.5 points)\r\n",
        "We almost finished in our journey into TF-IDF land. Let's finally calculate the inversed document frequency. The formula for it is idf(t) = N/df where N is the total number of documents and df is the document frequency that we calculated before. But if N is some really large number, our IDF term will become huge. Thus, we are going to take a log of it.\r\n",
        "\r\n",
        "Finally, the formula that we are going to use is: idf(t) = log(N/(df + 1)). log here is a natural logarithm, i.e. with base e.\r\n",
        "\r\n",
        "If we try to think what IDF does, it gives more weight to rare words in the document collection and less to the words that appear in many documents. The assumtion is that the first type of words and my characteristic of a document that the second type.\r\n",
        "\r\n",
        "The final IDF vector should be of the length of our vocabulary.\r\n",
        "\r\n",
        "Implement the idf() function below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sq0-gi_pZ7JL"
      },
      "source": [
        "def idf(texts, df):\r\n",
        "    return ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPpkD8GmZ74T"
      },
      "source": [
        "train_idf = idf(get_preprocessed_synopses(train_set), train_df)\r\n",
        "print(train_idf.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr0eLCOSZ_DW"
      },
      "source": [
        "\r\n",
        "Finally, TF-IDF is just a multiplication of two terms that we calculated before:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20d0kt5_aADc"
      },
      "source": [
        "def tfidf(tf, idf):\r\n",
        "    return tf * idf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjzjYD5MaBo2"
      },
      "source": [
        "train_tfidf = tfidf(train_tf, train_idf)\r\n",
        "train_tfidf_norm = normalize(train_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnBjH5CAaEQo"
      },
      "source": [
        "Sanity check again. The most similar document should be 0. If it's different, check your previous code for mistakes!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRHK7N-0aFDq"
      },
      "source": [
        "cosine_similarity(train_tfidf_norm[0, :], train_tfidf_norm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LirPDJmMaGv1"
      },
      "source": [
        "Since we want to compare test texts, which are unseen, we need to have the same vocabulary size as our train matrix. Thus, we are going to calculate the TF term for each test text and we are going to use the IDF term from our training set. For now, we are just ignoring the words that are in the test set but not in the train set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hum8TtdEaIhx"
      },
      "source": [
        "test_tfidf = tfidf(test_tf, train_idf)\r\n",
        "test_tfidf_norm = normalize(test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i8Uyja7aLcZ"
      },
      "source": [
        "tfidf_pred = []\r\n",
        "for i, test_entry in enumerate(test_tfidf_norm):\r\n",
        "    most_similar = cosine_similarity(test_entry, train_tfidf_norm, k=1)\r\n",
        "    tfidf_pred.append(train_set[most_similar[0]].tags)\r\n",
        "    \r\n",
        "print(accuracy(tfidf_pred, gold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrSHjHmwRn45"
      },
      "source": [
        "### Task 1.7: Questions (1 point)\r\n",
        "\r\n",
        "**Where could we use TF-IDF in practice? Give some examples of tasks in NLP that benefit from it. Explain your reasoning.** <br>\r\n",
        "Answer: TODO \r\n",
        "\r\n",
        "**When should one use sparse embeddings instead of short dense embeddings?**<br>\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**How can we evaluate embeddings? What resources can we use for it? (think about word and document level embeddings)**<br>\r\n",
        "Answer: TODO\r\n",
        "\r\n",
        "**What are some of the limitations of embeddings?**<br>\r\n",
        "Answer: TODO\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKdQu0KLQLvi"
      },
      "source": [
        "## Task 2: Doc2Vec (1 points)\r\n",
        "\r\n",
        "TF-IDF maybe a good solution for short query but it is most likely inefficient for long queries. This is because TF-IDF is a bag-of-words (BOW) algorithm. As the name suggests, all the words are just compiled in out vocabulary, like in a bag, and the order in which they appear in text does not affect the resulting vector. Thus Mary likes John and John likes Mary are going to have identical TF-IDF vectors. Another problem is that is the size of the vocabulary is large, we are going to have many zeros that don't represent anything useful.\r\n",
        "\r\n",
        "Another algorithm is Doc2Vec. This algorithm was described by the same people that invented Word2Vec in their paper Le, Q., & Mikolov, T. (2014, January). Distributed representations of sentences and documents. In International conference on machine learning (pp. 1188-1196)..\r\n",
        "\r\n",
        "The main idea is very similar to the Word2Vec model. Actually, the models are almost identical, with the only difference being a paragraph matrix added to the input. This matrix is going to be the embedding for the document.\r\n",
        "\r\n",
        "In this homework, we are going to train a Doc2Vec model using Gensim package.\r\n",
        "\r\n",
        "First, we need to prepare train and test sets. Train set should be a collection of TaggedDocument objects, that contain the text and the index of each text. This index is going to correspond to the additional paragraph matrix row, containing the document vector. The test set is just a list of texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVRTXRmpQVbs"
      },
      "source": [
        "doc2vec_train = []\r\n",
        "\r\n",
        "for i, text in enumerate(get_preprocessed_synopses(train_set)):\r\n",
        "    doc2vec_train.append(TaggedDocument(text, [i]))\r\n",
        "    \r\n",
        "doc2vec_test = [text for text in get_preprocessed_synopses(test_set)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rFhTdOLTQWTD"
      },
      "source": [
        "Next, to train the model, we just need to specify hyperparameters, build the vocabulary and run the train() function.\r\n",
        "\r\n",
        "Your task is to read the Doc2Vec documentation, Doc2Vec tutorial, and the paper above. Then, find the hyperparameters that, in your opinion, suit best our dataset.\r\n",
        "\r\n",
        "Leave the seed as 1234!\r\n",
        "\r\n",
        "Try to achieve the accuracy > 0.55"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xysoalUlQZsa"
      },
      "source": [
        "model = Doc2Vec(..., seed=1234)\r\n",
        "\r\n",
        "model.build_vocab(doc2vec_train)\r\n",
        "\r\n",
        "model.train(doc2vec_train, total_examples=model.corpus_count, epochs=model.epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AJOZt5YQgsM"
      },
      "source": [
        "Sanity check that the most similar text in the training set is the text itself. The first index should be 0."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scs6O-s1Qj-U"
      },
      "source": [
        "vector = model.infer_vector(doc2vec_train[0].words)\r\n",
        "model.docvecs.most_similar([vector])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm4fuh4NQkzz"
      },
      "source": [
        "doc2vec_pred = []\r\n",
        "for i, test_entry in enumerate(doc2vec_test):\r\n",
        "    vector = model.infer_vector(test_entry)\r\n",
        "    most_similar = model.docvecs.most_similar([vector])\r\n",
        "    doc2vec_pred.append(train_set[most_similar[0][0]].tags)\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rym_IoSyQtY8"
      },
      "source": [
        "Try to achieve the accuracy over 0.55. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wnVOmohLQrqL"
      },
      "source": [
        "print(accuracy(doc2vec_pred, gold))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzSA6g1MQ0fz"
      },
      "source": [
        "## Task 3: Write your own plot (1 point)\r\n",
        "\r\n",
        "Try to come up with your own movie plot. Then run three different algorithms (TF, TF-IDF, Doc2Vec) to find similar plots from the collection. Your plot should not be very long, three sentences should be enough unless your imagination strives for more :)\r\n",
        "\r\n",
        "Answer the following question:\r\n",
        "\r\n",
        "**In your opinion, which algorithm performed better in this task? Explain your choice.**\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIXBVaMLRapF"
      },
      "source": [
        "Answer: TODO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qhih0TVpRBwG"
      },
      "source": [
        "# Put your title here\r\n",
        "my_title = \"The Last Meow\"\r\n",
        "\r\n",
        "# Put your plot here\r\n",
        "my_plot = \"\"\"It's been a long time since humans think they rule over the cats. \r\n",
        "They've been teasing and humiliating them for too long. \r\n",
        "Now, the time has come for the cat army to rise and destroy the humanity once and for all.\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zusbn0zRChA"
      },
      "source": [
        "def preprocess(text):\r\n",
        "    doc = nlp(text)\r\n",
        "    preprocessed_text = synopsis = [token.lemma_.lower() for sent in doc.sents for token in sent \r\n",
        "                    if not token.is_stop and not token.is_punct and not token.is_space]\r\n",
        "    return preprocessed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R20plrfpREND"
      },
      "source": [
        "plot_preprocessed = preprocess(my_plot)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qAULLMaHRFng"
      },
      "source": [
        "my_plot_tf = tf([plot_preprocessed], vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUS54BUmRJIK"
      },
      "source": [
        "print(\"Similar plots by TF:\\n\")\r\n",
        "print(\"Author's title:\", my_title)\r\n",
        "print(\"Author's plot:\", my_plot)\r\n",
        "print(\"-\" * 50)\r\n",
        "for idx in cosine_similarity(normalize(my_plot_tf)[0], train_tf_norm, k=3):\r\n",
        "    print(f'Candidate {idx}:')\r\n",
        "    title = train_set[idx].title\r\n",
        "    print('Title:', title)\r\n",
        "    plot = train_set[idx].synopsis_original\r\n",
        "    print('Plot:', plot)\r\n",
        "    tags = train_set[idx].tags\r\n",
        "    print('Tags:', tags)\r\n",
        "    print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ct5BHjCURJz-"
      },
      "source": [
        "my_plot_tfidf = tfidf(my_plot_tf, train_idf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-M9yZjMRLRF"
      },
      "source": [
        "print(\"Similar plots by TF-IDF:\\n\")\r\n",
        "print(\"Author's title:\", my_title)\r\n",
        "print(\"Author's plot:\", my_plot)\r\n",
        "print(\"-\" * 50)\r\n",
        "for idx in cosine_similarity(normalize(my_plot_tfidf)[0], train_tf_norm, k=3):\r\n",
        "    print(f'Candidate {idx}:')\r\n",
        "    title = train_set[idx].title\r\n",
        "    print('Title:', title)\r\n",
        "    plot = train_set[idx].synopsis_original\r\n",
        "    print('Plot:', plot)\r\n",
        "    tags = train_set[idx].tags\r\n",
        "    print('Tags:', tags)\r\n",
        "    print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqkPfbEvRM4U"
      },
      "source": [
        "my_plot_vector = model.infer_vector(plot_preprocessed)\r\n",
        "print(\"Similar plots by TF-IDF:\\n\")\r\n",
        "print(\"Author's title:\", my_title)\r\n",
        "print(\"Author's plot:\", my_plot)\r\n",
        "print(\"-\" * 50)\r\n",
        "for idx in model.docvecs.most_similar([vector], topn=3):\r\n",
        "    idx = idx[0]\r\n",
        "    print(f'Candidate {idx}:')\r\n",
        "    title = train_set[idx].title\r\n",
        "    print('Title:', title)\r\n",
        "    plot = train_set[idx].synopsis_original\r\n",
        "    print('Plot:', plot)\r\n",
        "    tags = train_set[idx].tags\r\n",
        "    print('Tags:', tags)\r\n",
        "    print(\"-\" * 50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emviGkdqaOQu"
      },
      "source": [
        "### BONUS: Pointwise Mutual Information (1 point)\r\n",
        "\r\n",
        "For term-term-matrices we can use positive pointwise mutual information (PPMI or PMI in general). Here the vector dimensions correspond to words rather than documents. \r\n",
        "PMI normalizes the probability of the co-occurence of two words by their individual occrence probabilities: \r\n",
        "\r\n",
        "$$ PMI(w_1,w_2) = log_2 \\frac{P(w_1,w_2)}{P(w_1)P(w_2)} $$\r\n",
        "\r\n",
        "Here, $P(x)$ is a an estimate of the probability of word x, which we can calculate directly from a corpus using its frequency in a corpus. $P(w_1,w_2)$ is estimated probability that $w_1$ and $w_2$ co-occur in a corpus. In short, PMI checks if $w_1$ and $w_2$ co-occur more than they occur independently.\r\n",
        "\r\n",
        "Then the PMI between a target word and context word: \r\n",
        "$$ PMI(w_1,c) = log_2 \\frac{P(w_1,c)}{P(w_1)P(c)} $$\r\n",
        "\r\n",
        "PMI values range from -inf to +inf. Negative values indicate a co-occurence which is less likely to happen than by chance. As all the associations are computed based on sparse data and the definition of \"unrelated\" words is fuzzy, we usually ignore negative values by replacing them with 0, hence Psitive PMI:\r\n",
        "$$PPMI(w_1,c)=max(log_2\\frac{P(w_1,c)}{P(w_1)P(c)},0)$$\r\n",
        "\r\n",
        "**Your task is to calculate PPMI matrix using the data we have already read in from the previous exercises. You can use a small subset from the training split for this task.** You can get more information and an example from the book [Speech and Language Processing by Daniel Jurafsky and James H. Martin.](https://web.stanford.edu/~jurafsky/slp3/6.pdf)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0O5BTRtdL6Q"
      },
      "source": [
        "### Task 2.1: Calculating co-occurence counts \r\n",
        "\r\n",
        "First, you need to calculate the co-occurence matrix. Use a context window of your choice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZW74w9KXYEGc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkEhLJ1TZNLh"
      },
      "source": [
        "### Task 2.2: Calculate the joint probabilities \r\n",
        "Using the previously calculated co-occurence counts compute the joint probabilites. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb49VF_pZMhj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dasfWpBvns3t"
      },
      "source": [
        "### Task 2.3: Calculating PPMI matrix\r\n",
        "\r\n",
        "Now when you have calculated the joint probabilities, we can calculate the PPMI values. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4M28hzGYDLS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}