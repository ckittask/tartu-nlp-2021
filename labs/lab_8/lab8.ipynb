{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab8.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "154diMX4Hd9w"
      },
      "source": [
        "## Lab 8. Data Annotation Evaluation\n",
        "\n",
        "Welcome to Lab 8! \n",
        "\n",
        "Today we will be looking at how to evaluate the goodness of the annotations made by the raters. We call it inter-annotator agreement, which measures how well two or more annotators can make the same annotation decision for a certain category.  Well known measures are percentage agreement, Cohen's Kappa, Fleiss' Kappa, Krippendorff's Alpha. High inter-annotator reliability values refer to a high degree of agreement between two or more raters. Low inter-rater reliability values refer to a low degree of agreement between two or more raters. \n",
        "\n",
        "__Questions:__\n",
        "1. Why do we need to measure the reliability of the annotations? \n",
        "2. What could be the reason why annotators disagree? \n",
        "3. If we have two annotators and they both disagree on one sample, what class/label/etc should we use as the final class/label/etc? Should we prefer one annotator over the other? \n",
        "4. Why is it not a good idea to use percent agreement (no_of_agreements/total) to determine interrater reliability? \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3RaEG0sIOMo"
      },
      "source": [
        "### Cohen's Kappa \n",
        "\n",
        "[Cohen's kappa](https://en.wikipedia.org/wiki/Cohen%27s_kappa) ($\\kappa$) is coefficient  that is used to measure inter-rater reliability for categorical items. \n",
        "\n",
        "$$\\kappa = \\frac{p_o-p_e}{1-p_e}=1-\\frac{1-p_o}{1-p_e}$$\n",
        "Here, $p_o$ is relative observed agreement among raters. $p_e$ is the hypothetical probability of chance agreement. When raters completely agree with each other then $\\kappa=1$, if there is no agreement among the raters other than what would be expected by chance then $\\kappa=0$. \n",
        "\n",
        "NB! Cohen's kappa measures agreement between two raters only. If there are more than two raters, we would have to use some other measure. \n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*-epOFf318-TTB70lvFejzA.png\">\n",
        "\n",
        "Image from [Medium](https://towardsdatascience.com/cohens-kappa-9786ceceab58)\n",
        "\n",
        "\n",
        "Let's try to calculate Cohen's Kappa for our annotations. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fkebOysHzXG"
      },
      "source": [
        "### Annotation Data\n",
        "\n",
        "Let's first create some random and not so random annotation. For each case we have two annotators who classified samples into two classes - CORRECT and INCORRECT. We are going to calculate how reliabe the annotations are for both cases. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JFlwcGAMH7zH"
      },
      "source": [
        "import random \n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYGL6k-8CwNw"
      },
      "source": [
        "Let's visualize the annotations using confusion matrix. With confusion matrix, we can see how much the annotators agreed and disagreed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qK_wQXlPJjFR"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2hvEDCTDfnF"
      },
      "source": [
        "good_cm = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJz-9k64O0Wl"
      },
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt     \n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(..., annot=True, fmt='g', ax=ax); \n",
        "\n",
        "\n",
        "ax.set_xlabel('Annotator 2')\n",
        "ax.set_ylabel('Annotator 1')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(['CORRECT', 'INCORRECT'])\n",
        "ax.yaxis.set_ticklabels(['CORRECT', 'INCORRECT'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRP1BUa1DoH8"
      },
      "source": [
        "ax= plt.subplot()\n",
        "sns.heatmap(..., annot=True, fmt='g', ax=ax); \n",
        "\n",
        "\n",
        "ax.set_xlabel('Annotator 2')\n",
        "ax.set_ylabel('Annotator 1')\n",
        "ax.set_title('Confusion Matrix')\n",
        "ax.xaxis.set_ticklabels(['CORRECT', 'INCORRECT'])\n",
        "ax.yaxis.set_ticklabels(['CORRECT', 'INCORRECT'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17C812l9Pp4k"
      },
      "source": [
        "Let's first calculate the observed proportionate agreement $p_o$: \n",
        "\n",
        "$$p_o=\\frac{\\textrm{no in agreement}}{\\textrm{total}}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "verNgLeoPeaH"
      },
      "source": [
        "random_p_o = ...\n",
        "random_p_o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "weA93Q3WD67R"
      },
      "source": [
        "good_p_o = ...\n",
        "good_p_o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbKK568TPzY9"
      },
      "source": [
        "Now, let's calculate the probability of random agreement $p_e$. We first need to calculate the probabilities for each class separately (the probability that both raters would annotate as the same class at random): "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RG-kjgccPywb"
      },
      "source": [
        "random_p_correct = ...\n",
        "random_p_incorrect = ...\n",
        "print(random_p_incorrect, random_p_correct)\n",
        "random_p_e = ...\n",
        "print(random_p_e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVVp35JHEYyL"
      },
      "source": [
        "good_p_correct = ...\n",
        "good_p_incorrect = ...\n",
        "print(good_p_incorrect, good_p_correct)\n",
        "good_p_e = ...\n",
        "print(good_p_e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SRRNKFaQmb_"
      },
      "source": [
        "Now, we can calculate $\\kappa$: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpshVRC-Ql9m"
      },
      "source": [
        "random_kappa = ...\n",
        "print(random_kappa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "710YPy7vExse"
      },
      "source": [
        "good_kappa = ....\n",
        "print(good_kappa)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtdXalKnLgu_"
      },
      "source": [
        "What can we say?  \n",
        "\n",
        "| Kappa     | Agreement                  |\n",
        "|-----------|----------------------------|\n",
        "| < 0         | Less than chance agreement |\n",
        "| 0.01-0.20 | Slight agreement           |\n",
        "| 0.21-0.40 | Fair agreement             |\n",
        "| 0.41-0.60 | Moderate agreement         |\n",
        "| 0.61-0.80 | Substantial agreement      |\n",
        "| 0.81-0.99 | Almost perfect agreement   |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bgRuxZfIskJ"
      },
      "source": [
        "We can make our lives easier by using NLTK's implementation of Cohen's Kappa. Here is how to do that. NLTK provides other inter-annotator agreement coefficients like Krippendorff's Alpha and Fleiss' Kappa. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y0mIQZdhhcg7"
      },
      "source": [
        "from nltk.metrics.agreement import AnnotationTask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9SBHvVIhk4a"
      },
      "source": [
        "random_data = []\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXABINDvhgPY"
      },
      "source": [
        "random_task = ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLNRx-ftFBac"
      },
      "source": [
        "good_data = []\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okQzjVxUFI0w"
      },
      "source": [
        "good_task = "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}