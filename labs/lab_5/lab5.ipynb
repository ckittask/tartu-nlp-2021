{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "name": "lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEDYQfp6SkIf"
      },
      "source": [
        "# Lab 4. Text Classification with CNN\n",
        "\n",
        "In this lab, we are going to finally put all the previous knowledge into use to train our first neural NLP model. In particular, we are going to read the dataset, preprocess it and train a convolutional neural network using pretrained word vectors as inputs. \n",
        "\n",
        "Since we are going to build a deep network and we are going to have 25,000 texts, it's recommended that you run this notebook on a Cuda GPU. If you don't have one at your disposal, you can run this notebook on Google Colab.\n",
        "\n",
        "To do it, you just need to visit https://colab.research.google.com/ and upload and run this notebook there. Google Colab will allocate a GPU for you for about twelve hours or until you leave it inactive for some period of time.\n",
        "\n",
        "Also, if you are running this notebook on Google Colab, don't forget to go to `Runtime -> Change runtime type` and set `Runtime type` to `Python 3` and `Hardware acceleration` to `GPU`.\n",
        "\n",
        "This lab is based on [this tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb), so you can always visit it to get more information. However, we made some changes to the data loading part to make it more profound and flexible to use in other models. The original tutorial uses `torchtext` package to load the data. However, it is too high-level and it may be difficult to understand what is going under the hood. Additionaly, we will have more flexibility in adapting our own custom data loader to different tasks and datasets. \n",
        "\n",
        "With the setup being dealt with, we can proceed to building our classifier.\n",
        "\n",
        "## Text classification\n",
        "\n",
        "Text classification is one of the most popular NLP tasks. It can be used to predict a genre of a document, or establish the authorship of a text. In this lab, we are going to predict a sentiment of a sentence, i.e. try to guess if a text (in our case, a review) is positive or negative. \n",
        "\n",
        "Here is the decription of a CNN classifier from the tutorial above:\n",
        "\n",
        "> Traditionally, CNNs are used to analyse images and are made up of one or more convolutional layers, followed by one or more linear layers. The convolutional layers use filters (also called kernels or receptive fields) which scan across an image and produce a processed version of the image. This processed version of the image can be fed into another convolutional layer or a linear layer. Each filter has a shape, e.g. a 3x3 filter covers a 3 pixel wide and 3 pixel high area of the image, and each element of the filter has a weight associated with it, the 3x3 filter would have 9 weights. In traditional image processing these weights were specified by hand by engineers, however the main advantage of the convolutional layers in neural networks is that these weights are learned via backpropagation.\n",
        "\n",
        "> The intuitive idea behind learning the weights is that your convolutional layers act like feature extractors, extracting parts of the image that are most important for your CNN's goal, e.g. if using a CNN to detect faces in an image, the CNN may be looking for features such as the existance of a nose, mouth or a pair of eyes in the image.\n",
        "\n",
        "> So why use CNNs on text? In the same way that a 3x3 filter can look over a patch of an image, a 1x2 filter can look over a 2 sequential words in a piece of text, i.e. a bi-gram. In the previous tutorial we looked at the FastText model which used bi-grams by explicitly adding them to the end of a text, in this CNN model we will instead use multiple filters of different sizes which will look at the bi-grams (a 1x2 filter), tri-grams (a 1x3 filter) and/or n-grams (a 1x$n$ filter) within the text.\n",
        "\n",
        "> The intuition here is that the appearance of certain bi-grams, tri-grams and n-grams within the review will be a good indication of the final sentiment.\n",
        "\n",
        "## Data\n",
        "\n",
        "To train the classifier, we are going to use the [Large Movie Review Dataset](https://ai.stanford.edu/~amaas/data/sentiment/). It contains 25,000 reviews for training and another 25,000 for testing. Both training and test sets contain 12,500 positive reviews and 12,500 negative reviews.\n",
        "\n",
        "In the next steps, we are going to build our own custom dataloader to load, preprocess, split, and batch the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mT16Bz_-LBmg"
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "# from: https://spacy.io/api/tokenizer\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "\n",
        "# Create a Tokenizer with the default settings for English\n",
        "# including punctuation rules and exceptions\n",
        "tokenizer = nlp.Defaults.create_tokenizer(nlp)\n",
        "\n",
        "# Check if we are running on a CPU or GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M1EfPLgYwmb"
      },
      "source": [
        "Run the cell below if you want to save the files and trained models on your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBwcjCdvShJ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d446b00-557c-4a46-c366-a092cabb90f4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdftAyx8ZBRu"
      },
      "source": [
        "Let's download the vector file and unpack in to the `vector_cache/` folder. You can skip this step if you have already done it yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7WKqu2yMcXr"
      },
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlLJUBe6Mx7z"
      },
      "source": [
        "!unzip wiki-news-300d-1M.vec.zip -d vector_cache/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CETGAQ_YZ3mW"
      },
      "source": [
        "Let's download the dataset and unpack in to the `data/` folder. You can skip this step if you have already done it yourself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cXrxb9WNQfq"
      },
      "source": [
        "!wget https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIbXCHFHNg1b"
      },
      "source": [
        "!mkdir data/\n",
        "!tar -xzf aclImdb_v1.tar.gz -C data/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OyMQ_KEaMfR"
      },
      "source": [
        "We are going to define some variables that we are going to need later. \n",
        "\n",
        "We will need the `<PAD>` and `<UNK>` symbols. `<PAD>` is needed to make the sentences in one batch have the same length. We are going to prepend this symbol to the end of each sentence to equalize the lengths. `<UNK>` is needed to replace the words for which we don't have a pretrained vector.\n",
        "\n",
        "We are also going to define the paths for our vector file and data folder, as well as maximum numer of vectors that we want to store."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "md73ChlvLBnc"
      },
      "source": [
        "PAD = '<PAD>'\n",
        "PAD_ID = 0\n",
        "UNK = '<UNK>'\n",
        "UNK_ID = 1\n",
        "VOCAB_PREFIX = [PAD, UNK]\n",
        "\n",
        "VEC_PATH = Path('vector_cache') /  'wiki-news-300d-1M.vec'\n",
        "DATA_PATH = Path('data') / 'aclImdb'\n",
        "MAX_VOCAB = 25000\n",
        "\n",
        "batch_size = 64\n",
        "validation_split = .3\n",
        "shuffle_dataset = True\n",
        "random_seed = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yUjLPW1bK-q"
      },
      "source": [
        "First, let's prepare a vocabulary for our pretrained vectors. Since the input to our model should be an index of a word, we need to build it to map from words to indices.\n",
        "\n",
        "Below, we define a `PretrainedWordVocab` class that is going to take a list of words and build a vocab based on it. We also define some methods that we are going to use:\n",
        "\n",
        "- `normalize_unit()` to put the word to lowercase if `lower` argument is set to `True`.\n",
        "- `unit2id()` to return the index of a word in the vocab or an `<UNK>` index otherwise.\n",
        "- `id2unit()` to return a word given its index in the vocab.\n",
        "- `map()` to return a list of indeces given a list of words.\n",
        "- `build_vocab()` to initialize the vocab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB0tPL5DLBnn"
      },
      "source": [
        "class PretrainedWordVocab:\n",
        "    def __init__(self, data, lower=False):\n",
        "        self.data = data\n",
        "        self.lower = lower\n",
        "        self.build_vocab()\n",
        "        \n",
        "    def normalize_unit(self, unit):\n",
        "        if self.lower:\n",
        "            return unit.lower()\n",
        "        else:\n",
        "            return unit\n",
        "        \n",
        "    def unit2id(self, unit):\n",
        "        unit = self.normalize_unit(unit)\n",
        "        if unit in self._unit2id:\n",
        "            return self._unit2id[unit]\n",
        "        else:\n",
        "            return self._unit2id[UNK]\n",
        "    \n",
        "    def id2unit(self, id):\n",
        "        return self._id2unit[id]\n",
        "    \n",
        "    def map(self, units):\n",
        "        return [self.unit2id(unit) for unit in units]\n",
        "        \n",
        "    def build_vocab(self):\n",
        "        # self._id2unit - id to unit (add PAD and UNK)\n",
        "        # self._unit2id - unit to id \n",
        "        ...\n",
        "\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self._unit2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maj8w5GEcykb"
      },
      "source": [
        "Next, we need to create the `Pretrain` class to store the pretrained vectors and vocab that we defined above. The vectors are going to be stored in as a numpy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ryz6vtNoLBnt"
      },
      "source": [
        "class Pretrain:\n",
        "    def __init__(self, vec_filename, max_vocab=-1):\n",
        "        self._vec_filename = vec_filename\n",
        "        self._max_vocab = max_vocab\n",
        "        \n",
        "    @property\n",
        "    def vocab(self):\n",
        "        if not hasattr(self, '_vocab'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._vocab\n",
        "    \n",
        "    @property\n",
        "    def emb(self):\n",
        "        if not hasattr(self, '_emb'):\n",
        "            self._vocab, self._emb = self.read()\n",
        "        return self._emb\n",
        "        \n",
        "    def read(self):\n",
        "        if self._vec_filename is None:\n",
        "            raise Exception(\"Vector file is not provided.\")\n",
        "        print(f\"Reading pretrained vectors from {self._vec_filename}...\")\n",
        "        \n",
        "        words, emb, failed = self.read_from_file(self._vec_filename, open_func=open)\n",
        "        \n",
        "        if failed > 0: # recover failure\n",
        "            emb = emb[:-failed]\n",
        "        if len(emb) - len(VOCAB_PREFIX) != len(words):\n",
        "            raise Exception(\"Loaded number of vectors does not match number of words.\")\n",
        "            \n",
        "        # Use a fixed vocab size\n",
        "        if self._max_vocab > len(VOCAB_PREFIX) and self._max_vocab < len(words):\n",
        "            words = words[:self._max_vocab - len(VOCAB_PREFIX)]\n",
        "            emb = emb[:self._max_vocab]\n",
        "                \n",
        "        vocab = PretrainedWordVocab(words, lower=True)\n",
        "        \n",
        "        return vocab, emb\n",
        "        \n",
        "    def read_from_file(self, filename, open_func=open):\n",
        "        \"\"\"\n",
        "        Open a vector file using the provided function and read from it.\n",
        "        \"\"\"\n",
        "        first = True\n",
        "        words = []\n",
        "        failed = 0\n",
        "        with open_func(filename, 'rb') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                try:\n",
        "                    line = line.decode()\n",
        "                except UnicodeDecodeError:\n",
        "                    failed += 1\n",
        "                    continue\n",
        "                if first:\n",
        "                    # the first line contains the number of word vectors and the dimensionality\n",
        "                    first = False\n",
        "                    line = line.strip().split(' ')\n",
        "                    rows, cols = [int(x) for x in line]\n",
        "                    emb = np.zeros((rows + len(VOCAB_PREFIX), cols), dtype=np.float32)\n",
        "                    continue\n",
        "\n",
        "                line = line.rstrip().split(' ')\n",
        "                emb[i+len(VOCAB_PREFIX)-1-failed, :] = [float(x) for x in line[-cols:]]\n",
        "                words.append(' '.join(line[:-cols]))\n",
        "        return words, emb, failed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ktSIbcqdKWO"
      },
      "source": [
        "Finally, we need to define the dataset class `IMDBDataSet` that is going to load and preprocess our data files. Inside the data folder, we have `train` and `test` folders that have `neg` and `pos` folders inside of then. Each of these folders have a review as a separate file.\n",
        "\n",
        "We are going to iterate through each file inside these folders, read the text, tokenize it with [Spacy tokenizer](https://spacy.io/api/tokenizer) and replace the words with the indices using the `PretrainedWordVocab` that we created earlier.\n",
        "\n",
        "We also need our custom class to inherit from the `torch.utils.data.Dataset` class. Finally, we need to define the `__len__()` method to know how big is our dataset and `__getitem__()` method to get one sample at a given index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHnV7tRJLBnx"
      },
      "source": [
        "class IMDBDataSet(Dataset):\n",
        "    def __init__(self, pretrain, data_folder='.data', test=False):\n",
        "        self.pretrain_vocab = pretrain.vocab\n",
        "        self.label_vocab = {'neg': 0, 'pos': 1}\n",
        "        \n",
        "        if test:\n",
        "            self.data_folder = data_folder / 'test'\n",
        "        else:\n",
        "            self.data_folder = data_folder / 'train'\n",
        "            \n",
        "        self.data = []\n",
        "        \n",
        "        if self.data_folder.exists():\n",
        "            self.load()\n",
        "        else:\n",
        "            raise ValueError(\"Data path doesn't exist!\")\n",
        "        \n",
        "    def load(self):\n",
        "        for label in ['pos', 'neg']:\n",
        "            print(f'Reading {label} sentences...')\n",
        "            ...\n",
        "            \n",
        "                \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBTTvrcgewAE"
      },
      "source": [
        "Additionally, we need to define a funciton to pad all the sentences in the batch to the same length. To do this, we are going to first find the longest sequence in the batch and use its length to create a torch tensor of size `(batch_size, max_len)` filled with `0` that is our padding id. Later, we are just going to append each sequence in the beginning of the corresponding row of our new batch tensor. Don't forget that `nn.Embedding` layer that we are going to use later requires indices to be of type `long`. We are also going to put the labels, with `0` corresponding to `negative` and `1` to `positive` to the `labels` tensor of length `batch_size`. To be able to use them to calculate the loss, each label must be of type `float`.\n",
        "\n",
        "Finally, don't forget to convert all the tensors to the current device with `.to(device)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xe34jVx2LBn0"
      },
      "source": [
        "def pad_sequences(batch):\n",
        "  ...\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-O_HtNqgQfA"
      },
      "source": [
        "Now, we can finally load our data and pretrained vectors. It will take some time..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRPKP4YfLBn8"
      },
      "source": [
        "pretrain = Pretrain(VEC_PATH, MAX_VOCAB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCMyYcj1LBoC"
      },
      "source": [
        "train_data = IMDBDataSet(pretrain, DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U674r0iXLBoJ"
      },
      "source": [
        "test_data = IMDBDataSet(pretrain, DATA_PATH, test=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTiKgSxdhMkO"
      },
      "source": [
        "The last step in our data preparation is to define the train and validation splits. We are going to use the validation set to see how the model performs during the training. It is important to be able to see if the model is overfitting or not.\n",
        "\n",
        "To do that, we will just create a range of indices from `0` to the size of the training data. Then, we are going to define an index on which we are going to splite the data. Optionally, we can shuffle our indices before splitting.\n",
        "\n",
        "With these indices for train and validation datasets, we are going to create two corresponding `torch.utils.data.SubsetRandomSampler` objects that we are going to pass to the `torch.utils.data.DataLoader` objects in the next step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_6Y55v9LBn4"
      },
      "source": [
        "# Creating data indices for training and validation splits:\n",
        "dataset_size = len(train_data)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "if shuffle_dataset:\n",
        "    np.random.seed(random_seed)\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "# Creating PT data samplers and loaders:\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "valid_sampler = SubsetRandomSampler(val_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zuyq7dLjiRZ1"
      },
      "source": [
        "Here, for each set, we are going to create a `DataLoader` object that is going to create a batch iterator for us. We will pass to it out `IMDBDataSet` object as a source of data. Batch size as a `batch_size` argument. To specify train and validation splits, we are going to pass the corresponding `SubsetRandomSampler` objects as a `sampler` argument for the training set. Finally, we need to pass our `pad_sequences()` function as a `collate_fn` argument to tell the data loader how to prepare the batches so that they have the same length. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgn22jTLLBoN"
      },
      "source": [
        "train_loader = DataLoader() \n",
        "validation_loader = DataLoader()\n",
        "test_loader = DataLoader()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyqT79e3jL4r"
      },
      "source": [
        "The model descpition and the code below is taken from [the Build the Model section of this tutorial](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb). Please, refer to it for the necessary details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnEhkstjLBoQ"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self, pretrain, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
        "                 dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "                \n",
        "        self.embedding = nn.Embedding.from_pretrained(\n",
        "            torch.from_numpy(pretrain.emb), \n",
        "            padding_idx=pad_idx, \n",
        "            freeze=True\n",
        "        )\n",
        "        \n",
        "        self.convs = nn.ModuleList([\n",
        "                                    nn.Conv2d(in_channels = 1, \n",
        "                                              out_channels = n_filters, \n",
        "                                              kernel_size = (fs, embedding_dim)) \n",
        "                                    for fs in filter_sizes\n",
        "                                    ])\n",
        "        \n",
        "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text):           \n",
        "        #text = [batch size, sent len]\n",
        "\n",
        "        embedded = self.embedding(text)     \n",
        "        #embedded = [batch size, sent len, emb dim]\n",
        "        \n",
        "        embedded = embedded.unsqueeze(1)  \n",
        "        #embedded = [batch size, 1, sent len, emb dim]\n",
        "        \n",
        "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]    \n",
        "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
        "                \n",
        "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]     \n",
        "        #pooled_n = [batch size, n_filters]\n",
        "        \n",
        "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
        "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
        "            \n",
        "        return self.fc(cat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrzLk7fTLBoV"
      },
      "source": [
        "INPUT_DIM = len(pretrain.vocab)\n",
        "EMBEDDING_DIM = pretrain.emb.shape[1]\n",
        "N_FILTERS = 100\n",
        "FILTER_SIZES = [3,4,5]\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.5\n",
        "\n",
        "model = CNN(pretrain, INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtCisPr9LBoc"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0sYlvYpLBoi"
      },
      "source": [
        "def binary_accuracy(preds, y): \n",
        "    \"\"\"\n",
        "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
        "    \"\"\"\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEvv1JenLBon"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    for batch in iterator:\n",
        "\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yw_-9W8LBot"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    ...\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzP7wNcALBoz"
      },
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sSmBPJNqLBo5"
      },
      "source": [
        "N_EPOCHS = 5\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, validation_loader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'imdb_cnn_classifier.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3C-LI6oGrRF-"
      },
      "source": [
        "start_time = time.time()\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "end_time = time.time()\n",
        "epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "\n",
        "print(f'Epoch: test | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "print(f'\\tTest Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ2k7s81Dony"
      },
      "source": [
        "## User input\n",
        "\n",
        "Once we trained our model, we can try to predict the sentiment of our own input. We are going to define the `predict_sentiment()` function that is going to take our trained model and a sentence as an argument. \n",
        "\n",
        "First, we need to switch the model to evaluation mode be calling `model.eval()` on it. Then, we are going to tokenize the sentence the same way as we tokenized the input. If the sentence is less than `min_len` parameter, we are going to add the padding symbols to it, so our model doesn't throw an error. After that, we turn the words into indices with the same vocabulary that we built for training. Finally, we transform the output into tenson and adding an empty dimention in the beginning, imitating a batch of size 1.\n",
        "\n",
        "As we remember from the training part, `0` was a negative sentiment and `1` was positive. Thus, the closer to `0` is our prediction, the more negative is the sentiment and the opposite is true for positive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CS4ZMuw-LBo_"
      },
      "source": [
        "def predict_sentiment(model, sentence, min_len = 5):\n",
        "    ..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aToaCvbyC7fW"
      },
      "source": [
        "predict_sentiment(model, \"This film is so bad that I had to wash my eyes with bleach after watching it\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HF7hS4-FC_Qe"
      },
      "source": [
        "predict_sentiment(model, \"After watching this movie, I felt that I'm in heaven\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2WkdWEoFB6Z"
      },
      "source": [
        "# References\n",
        "\n",
        "- [Github tutorial on CNN](https://github.com/bentrevett/pytorch-sentiment-analysis/blob/master/4%20-%20Convolutional%20Sentiment%20Analysis.ipynb)\n",
        "- [Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., & Potts, C. (2011, June). Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies-volume 1 (pp. 142-150). Association for Computational Linguistics.](https://dl.acm.org/doi/10.5555/2002472.2002491)\n",
        "- [Joulin, A., Grave, E., Bojanowski, P., Douze, M., Jégou, H., & Mikolov, T. (2016). Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.](https://arxiv.org/abs/1612.03651)\n"
      ]
    }
  ]
}