{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XqEdofUHCwa"
      },
      "source": [
        "# Lab 4: Word Embeddings\r\n",
        "\r\n",
        "Welcome to lab 4! In todays lab we will be looking how to represent a word as a dense vectors. In the homework you will be learn more about the long sparse vectors. \r\n",
        "\r\n",
        "Word embeddings are popular way of representing text data in problems that are solved by deep learning algorithms. It provides a short dense representaion of a word filled with floating numbers. The hypothesis behind word embeddings is simple: words that occur in the same contexts tend to have similar meanings. \r\n",
        "\r\n",
        "\r\n",
        "## Initializing Embeddings\r\n",
        "\r\n",
        "One way to create word embeddings is to start with dense vectors for each token containing random numbers, and then train a model such as a document classifier. After training you will end up with the trained embeddings and model. \r\n",
        "\r\n",
        "\r\n",
        "PyTorch has a class for that called Embedding, which is a simple lookup table that stores embeddings of a fixed dictionary and size. You can initialize them randomly or from a pretrained embeddings. \r\n",
        "\r\n",
        "To initialize the embedding we need to define the dimension of the vector.Usually the dimension varies according to the vocabulary size. It is quite common to use a word embedding of dimension size 50, 100, 256, 300 and sometimes 1000. As the dimension size is a hyper-parameter, we need to play with it during the training phase.\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7lgSESIpUm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ef77dfe-f022-458a-eaa3-f82529bfde78"
      },
      "source": [
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "\r\n",
        "torch.manual_seed(1234)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fe31c725970>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSMimY6DIyNK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed5d1aae-4baf-4c0b-a31a-94d6f207a2c5"
      },
      "source": [
        "word_to_ix = {\"natural\":0, \"language\":1, \"processing\":2}\r\n",
        "word_to_ix"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'language': 1, 'natural': 0, 'processing': 2}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfe0zNR4I4kJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aab4dadc-bbcb-4fe6-df6b-b601790d0966"
      },
      "source": [
        "embeddings = ... # three words in vocab, 5 dimensional embeddings\r\n",
        "embeddings "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJSsQtl0JByg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff82efe1-d9a1-4669-d006-dc87bfcfec96"
      },
      "source": [
        "# lets create a lookup tensor for word \"natural\"\r\n",
        "lookup_tensor = ...\r\n",
        "lookup_tensor"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkJx-Le4JM4x"
      },
      "source": [
        "The following sets up an embedding layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DM3PyxAJMWD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90837fb1-f642-483d-8d6e-38409551b5f5"
      },
      "source": [
        "emb_layer = ...\r\n",
        "emb_layer"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ur1TvVRzIxgz"
      },
      "source": [
        "### Loading pretrained word embeddings \r\n",
        "\r\n",
        "Training your own word embeddings would be useful when we are working in specific domains such as medicine and manufacturing, where we have lot of data to train the embeddings. When we have little data on which we cannot meaningfully train the embeddings, we can use embeddings, which are trained on different data corpuses such as Wikipedia, Google News etc. \r\n",
        "\r\n",
        "There are many pretrained word embeddings available: Word2Vec, fastText, GloVe, ELMo. We can use these embeddings to initialize the weights instead of initializing them randomly. \r\n",
        "\r\n",
        "When you download the pretrained word embeddings, they usually look like this: \r\n",
        "\r\n",
        "say_VERB -0.008861 0.097097 0.100236 0.070044 -0.079279 0.000923 -0.012829 0.064301 -0.029405 -0.009858 ...<br>\r\n",
        "go_VERB 0.010490 0.094733 0.143699 0.040344 -0.103710 -0.000016 -0.014351 0.019653 0.069472 -0.046938 ...<br>\r\n",
        "make_VERB -0.013029 0.038892 0.008581 0.056925 -0.100181 0.011566 -0.072478 0.156239 0.038442 -0.073817 ... <br>\r\n",
        "thirty-six_NUM 0.058545 0.089598 0.052056 0.013421 -0.022304 -0.056648 -0.017670 0.095910 -0.028729 ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sqQeiU5V8HpT",
        "outputId": "521ae50b-6ca9-4e17-a953-6921108306c4"
      },
      "source": [
        "weight = torch.FloatTensor([[1, 1.2, 3,], [5,1.4,3.2]])\r\n",
        "embedding = ...\r\n",
        "input = ... # get for index 1\r\n",
        "..."
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkzRI-jv8hOY"
      },
      "source": [
        "We can download embeddings with torchtext.vocab: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhBT3HIQ8p0l"
      },
      "source": [
        "from torchtext.vocab import GloVe \r\n",
        "..."
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZACYQjJHyu4"
      },
      "source": [
        "Let's look inside: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WPilfje8sEj",
        "outputId": "681de172-35b2-49b4-ffbc-eaad8376b029"
      },
      "source": [
        "..."
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlCbCS0E6o2k"
      },
      "source": [
        "## Training Word Embeddings\r\n",
        "\r\n",
        "We can train our own word embeddings using different algorithms. Word2Vec provides two different algorithms: Continuous Bag-Of-Words and Skip-Gram. Both are shown graphically in the image below. \r\n",
        "<img src=\"https://miro.medium.com/max/2400/1*cuOmGT7NevP9oJFJfVpRKA.png\">\r\n",
        "Continues Bag-Of-Words predicts the center word given the context. Skip-Gram predicts the context words given the center word as an input. \r\n",
        "\r\n",
        "You can read more about these algorithms from the original article: [Tomas Mikolov et al: Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov et al: Distributed Representations of Words and Phrases and their Compositionality.](https://arxiv.org/abs/1310.4546)\r\n",
        "\r\n",
        "We can use Gensim to train Word2Vec embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNq-4IPiFoER"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "import spacy\r\n",
        "nlp = spacy.load('en_core_web_sm')\r\n",
        "\r\n",
        "data = ...\r\n",
        "doc = ..."
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KzL8zALIdvI"
      },
      "source": [
        "texts = ..."
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GisDxBqJI8p9"
      },
      "source": [
        "texts[]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XUmkhMVRGm-w"
      },
      "source": [
        "import multiprocessing\r\n",
        "cores = multiprocessing.cpu_count()\r\n",
        "\r\n",
        "model = ..."
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTwycrDMKJV4"
      },
      "source": [
        "Now we need to build the vocabulary table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUeMbHkHKObY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12afdb57-58ec-4aaf-c756-1285f4bc8539"
      },
      "source": [
        "..."
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPdTTIDlKdEO"
      },
      "source": [
        "Let's train the model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Ky2AUhwKgAf",
        "outputId": "d858d52a-3f2c-4c87-dc64-0cc015e289aa"
      },
      "source": [
        "..."
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cNgPBZYKwSL"
      },
      "source": [
        "model.init_sims(replace=True) # saves memory (cannot continue training after doing)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_5z7c2-K0qP"
      },
      "source": [
        "Let's expore this model. We can ask the model what are the similar words: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bismiK0LBfe",
        "outputId": "420c9870-12ed-4813-c3c8-6aa0a7b7fffe"
      },
      "source": [
        "..."
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmdVCaF7LExb",
        "outputId": "dec02333-71c6-4d8d-946c-fc3b19a5eaa5"
      },
      "source": [
        "..."
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4K8bihOSLKwU",
        "outputId": "2b4e450f-47a9-4939-96d6-2cab551560ec"
      },
      "source": [
        "# odd one out\r\n",
        "..."
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRUuAQCvgL9H"
      },
      "source": [
        "## Issues with Word2Vec embeddings\r\n",
        "\r\n",
        "As, Word2Vec creates emebdding for each word seen in the training data, it cannot handle words that it did not encounter during training. This leaves us with many out of vocabulary words. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAFmwzX0goUG"
      },
      "source": [
        "# not existing in the the embeddings \r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_SXtYqNgt_q"
      },
      "source": [
        "Words can have many senses, meaning that depending on the context, word can take on different meanings. Let's consider the word *nail*. It could be a upper surface of the tip of the finger or small metal spike. Word2Vec only learns one representation of this word. \r\n",
        "\r\n",
        "**What can we use instead of that? **"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYVdkaLShnXF"
      },
      "source": [
        "Another issue is with morphologically rich languages. \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GrsiM-l2h5Vh"
      },
      "source": [
        "## FastText\r\n",
        "\r\n",
        "To solve all of those issues, Bojanowski et al. proposed a new embedding methods called FastText. The main idea of FasText is the use of the internal structure of a word to improve vector representation obtained from skip-gram. \r\n",
        "\r\n",
        "Let's go over a simple example. We have a sentence \"I am drinking coffee.\" and we need to predict the context words \"am\" and \"coffee\" from the center word \"drinking\". \r\n",
        "1. The center word is split into n-grams. Embedding fot the center word is the sum of the embeddings of the n-grams and the word itself. \r\n",
        "2. Context words embeddings are directly taken from the embedding table (no n-grams are added). \r\n",
        "3. Collect negative samples. \r\n",
        "4. Dot product between the center and context words is taken and then sigmoid function is applied to this dot product to get a match score between 0 and 1. \r\n",
        "5. Embeddings are updated based on the loss. This will bring the actual context words closer to the center words and further from the negative samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3Ka7rCxnprM",
        "outputId": "be41457d-bfcd-4597-cb40-b3599daa9d22"
      },
      "source": [
        "from gensim.models.fasttext import FastText\r\n",
        "model = ...\r\n",
        "\r\n",
        "# build the vocabulary\r\n",
        "...\r\n",
        "\r\n",
        "# train the model\r\n",
        "\r\n",
        "..."
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ellipsis"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNyoDQIaoAR5"
      },
      "source": [
        "wv = ..."
      ],
      "execution_count": 20,
      "outputs": []
    }
  ]
}